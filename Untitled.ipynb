{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb059ffd-30ad-416e-92a0-84ad6e4ac481",
   "metadata": {},
   "source": [
    "# FINN - End-to-End Flow\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "In this notebook, we will show how to take a simple, binarized, fully-connected network trained on the CIFAR10 data set and take it all the way down to a customized bitfile running on a PYNQ board. \n",
    "\n",
    "This notebook is quite lengthy, and some of the cells (involving Vivado synthesis) may take up to an hour to finish running. To let you save and resume your progress, we will save the intermediate ONNX models that are generated in the various steps to disk, so that you can jump back directly to where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "064312a4-1fed-4651-ae50-ecaf8cb25309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau\n",
    "from finn.util.visualization import showSrc, showInNetron\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2240a1b7-e03a-4d6a-8d88-7062bc1ad691",
   "metadata": {},
   "source": [
    "## 1. Brevitas export <a id='brev_exp'></a>\n",
    "FINN expects an ONNX model as input. This can be a model trained with [Brevitas](https://github.com/Xilinx/brevitas). Brevitas is a PyTorch library for quantization-aware training and the FINN Docker image comes with several [example Brevitas networks](https://github.com/Xilinx/brevitas/tree/master/src/brevitas_examples/bnn_pynq). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525cb044-be47-4dca-be56-6b0cba1390de",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b8bdc2-66cd-4076-be27-e1712c9d9c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "class GetTransforms():\n",
    "    '''Returns a list of transformations when type as requested amongst train/test\n",
    "       Transforms('train') = list of transforms to apply on training data\n",
    "       Transforms('test') = list of transforms to apply on testing data'''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def trainparams(self):\n",
    "        train_transformations = [ #resises the image so it can be perfect for our model.\n",
    "            transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
    "            transforms.RandomRotation((-7,7)),     #Rotates the image to a specified angel\n",
    "            transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n",
    "            transforms.ToTensor(), # comvert the image to tensor so that it can work with torch\n",
    "            transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261)) #Normalize all the images\n",
    "            ]\n",
    "\n",
    "        return train_transformations\n",
    "\n",
    "    def testparams(self):\n",
    "        test_transforms = [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261))\n",
    "        ]\n",
    "        return test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "863f2273-af16-462b-b093-28106bfe5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = GetTransforms()\n",
    "train_transforms = transforms.Compose(transformations.trainparams())\n",
    "test_transforms = transforms.Compose(transformations.testparams())\n",
    "\n",
    "\n",
    "class GetCIFAR10_TrainData():\n",
    "    def __init__(self, dir_name:str):\n",
    "        self.dirname = dir_name\n",
    "\n",
    "    def download_train_data(self):\n",
    "        return datasets.CIFAR10('resnet18/data', train=True, download=True, transform=train_transforms)\n",
    "\n",
    "    def download_test_data(self):\n",
    "        return datasets.CIFAR10('resnet18/data', train=False, download=True, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e55716-6d26-42d3-8446-057921fda013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data = GetCIFAR10_TrainData(os.chdir(\"..\"))\n",
    "trainset = data.download_train_data()\n",
    "testset = data.download_test_data()\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=592,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=592,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a95e50-bdcd-420c-84b9-e2dd1f1e5693",
   "metadata": {},
   "source": [
    "### Define a PyTorch Device\n",
    "\n",
    "GPUs can significantly speed-up training of deep neural networks. We check for availability of a GPU and if so define it as target device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76948f12-ae25-42d6-b852-d0714829a27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083293d7-48f9-4636-b3cc-643e348e3a43",
   "metadata": {},
   "source": [
    "### Define the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd007e3-0c8e-4d05-9b93-c59b38828dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model define\n"
     ]
    }
   ],
   "source": [
    "from brevitas.nn import QuantConv2d, QuantLinear\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "weight_bit_width = 8\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, dropout=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = QuantConv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = QuantConv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                QuantConv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\"),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.dropout(out, p=self.dropout)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = F.dropout(out, p=self.dropout)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=self.dropout)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = QuantConv2d(in_planes, planes, kernel_size=1, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = QuantConv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = QuantConv2d(planes, self.expansion*planes, kernel_size=1, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                QuantConv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\"),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=200, dropout=0.0):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.conv1 = QuantConv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = QuantLinear(512*block.expansion, num_classes, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, dropout=self.dropout))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.dropout(out, p=self.dropout)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10, dropout=0.0):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], num_classes=num_classes, dropout=dropout)\n",
    "\n",
    "\n",
    "print(\"Model define\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c402bd3-7436-4068-ba23-96afa60c5224",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee3293b-83dc-43b1-aaf4-41f95dc21028",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"%s\" % i for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a8abd4-c1ca-4510-94bf-4790c6704bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, classes, test_losses, test_accs,\n",
    "         misclassified_imgs, correct_imgs, is_last_epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss +=criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            is_correct = pred.eq(target.view_as(pred))\n",
    "            if is_last_epoch:\n",
    "              misclassified_inds = (is_correct==0).nonzero()[:,0]\n",
    "              for mis_ind in misclassified_inds:\n",
    "                if len(misclassified_imgs) == 25:\n",
    "                  break\n",
    "                misclassified_imgs.append({\n",
    "                    \"target\": target[mis_ind].cpu().numpy(),\n",
    "                    \"pred\": pred[mis_ind][0].cpu().numpy(),\n",
    "                    \"img\": data[mis_ind]\n",
    "                })\n",
    "              \n",
    "              correct_inds = (is_correct==1).nonzero()[:,0]\n",
    "              for ind in correct_inds:\n",
    "                if len(correct_imgs) == 25:\n",
    "                  break\n",
    "                correct_imgs.append({\n",
    "                    \"target\": target[ind].cpu().numpy(),\n",
    "                    \"pred\": pred[ind][0].cpu().numpy(),\n",
    "                    \"img\": data[ind]\n",
    "                })\n",
    "            correct += is_correct.sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    if test_acc >= 90.0:\n",
    "        classwise_acc(model, device, test_loader, classes)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc3aec9-45d0-4cf6-adbc-32ba5deb623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classwise_acc(model, device, test_loader, classes):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    # print class-wise test accuracies\n",
    "    print()\n",
    "    for i in range(10):\n",
    "      print('Accuracy of %5s : %2d %%' % (\n",
    "          classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00440e15-8ed1-4431-b32f-876390a37583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1543471d-30f6-4f13-8801-61e4d037524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function ---\n",
    "def remove_export_handlers(model):\n",
    "    count = 0\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, \"export_handler\"):\n",
    "            module.export_handler = None\n",
    "            count += 1\n",
    "    print(f\"✅ Removed export_handler from {count} Quant layers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c73370cc-f279-4cd9-b02f-d654bd5d59b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Removed export_handler from 105 Quant layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): QuantConv2d(\n",
       "    3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): QuantConv2d(\n",
       "          64, 128, kernel_size=(1, 1), stride=(2, 2)\n",
       "          (input_quant): ActQuantProxyFromInjector(\n",
       "            (_zero_hw_sentinel): StatelessBuffer()\n",
       "          )\n",
       "          (output_quant): ActQuantProxyFromInjector(\n",
       "            (_zero_hw_sentinel): StatelessBuffer()\n",
       "          )\n",
       "          (weight_quant): WeightQuantProxyFromInjector(\n",
       "            (_zero_hw_sentinel): StatelessBuffer()\n",
       "            (tensor_quant): RescalingIntQuant(\n",
       "              (int_quant): IntQuant(\n",
       "                (float_to_int_impl): RoundSte()\n",
       "                (tensor_clamp_impl): TensorClampSte()\n",
       "                (delay_wrapper): DelayWrapper(\n",
       "                  (delay_impl): _NoDelay()\n",
       "                )\n",
       "              )\n",
       "              (scaling_impl): StatsFromParameterScaling(\n",
       "                (parameter_list_stats): _ParameterListStats(\n",
       "                  (first_tracked_param): _ViewParameterWrapper(\n",
       "                    (view_shape_impl): OverTensorView()\n",
       "                  )\n",
       "                  (stats): _Stats(\n",
       "                    (stats_impl): AbsMax()\n",
       "                  )\n",
       "                )\n",
       "                (stats_scaling_impl): _StatsScaling(\n",
       "                  (affine_rescaling): Identity()\n",
       "                  (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                    (clamp_min_ste): ScalarClampMinSte()\n",
       "                    (restrict_value_impl): FloatRestrictValue()\n",
       "                  )\n",
       "                  (restrict_scaling_pre): Identity()\n",
       "                )\n",
       "              )\n",
       "              (int_scaling_impl): IntScaling()\n",
       "              (zero_point_impl): ZeroZeroPoint(\n",
       "                (zero_point): StatelessBuffer()\n",
       "              )\n",
       "              (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "                (bit_width): StatelessBuffer()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (bias_quant): BiasQuantProxyFromInjector(\n",
       "            (_zero_hw_sentinel): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): QuantConv2d(\n",
       "          128, 256, kernel_size=(1, 1), stride=(2, 2)\n",
       "          (input_quant): ActQuantProxyFromInjector(\n",
       "            (_zero_hw_sentinel): StatelessBuffer()\n",
       "          )\n",
       "          (output_quant): ActQuantProxyFromInjector(\n",
       "            (_zero_hw_sentinel): StatelessBuffer()\n",
       "          )\n",
       "          (weight_quant): WeightQuantProxyFromInjector(\n",
       "            (_zero_hw_sentinel): StatelessBuffer()\n",
       "            (tensor_quant): RescalingIntQuant(\n",
       "              (int_quant): IntQuant(\n",
       "                (float_to_int_impl): RoundSte()\n",
       "                (tensor_clamp_impl): TensorClampSte()\n",
       "                (delay_wrapper): DelayWrapper(\n",
       "                  (delay_impl): _NoDelay()\n",
       "                )\n",
       "              )\n",
       "              (scaling_impl): StatsFromParameterScaling(\n",
       "                (parameter_list_stats): _ParameterListStats(\n",
       "                  (first_tracked_param): _ViewParameterWrapper(\n",
       "                    (view_shape_impl): OverTensorView()\n",
       "                  )\n",
       "                  (stats): _Stats(\n",
       "                    (stats_impl): AbsMax()\n",
       "                  )\n",
       "                )\n",
       "                (stats_scaling_impl): _StatsScaling(\n",
       "                  (affine_rescaling): Identity()\n",
       "                  (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                    (clamp_min_ste): ScalarClampMinSte()\n",
       "                    (restrict_value_impl): FloatRestrictValue()\n",
       "                  )\n",
       "                  (restrict_scaling_pre): Identity()\n",
       "                )\n",
       "              )\n",
       "              (int_scaling_impl): IntScaling()\n",
       "              (zero_point_impl): ZeroZeroPoint(\n",
       "                (zero_point): StatelessBuffer()\n",
       "              )\n",
       "              (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "                (bit_width): StatelessBuffer()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (bias_quant): BiasQuantProxyFromInjector(\n",
       "            (_zero_hw_sentinel): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): QuantConv2d(\n",
       "          256, 512, kernel_size=(1, 1), stride=(2, 2)\n",
       "          (input_quant): ActQuantProxyFromInjector(\n",
       "            (_zero_hw_sentinel): StatelessBuffer()\n",
       "          )\n",
       "          (output_quant): ActQuantProxyFromInjector(\n",
       "            (_zero_hw_sentinel): StatelessBuffer()\n",
       "          )\n",
       "          (weight_quant): WeightQuantProxyFromInjector(\n",
       "            (_zero_hw_sentinel): StatelessBuffer()\n",
       "            (tensor_quant): RescalingIntQuant(\n",
       "              (int_quant): IntQuant(\n",
       "                (float_to_int_impl): RoundSte()\n",
       "                (tensor_clamp_impl): TensorClampSte()\n",
       "                (delay_wrapper): DelayWrapper(\n",
       "                  (delay_impl): _NoDelay()\n",
       "                )\n",
       "              )\n",
       "              (scaling_impl): StatsFromParameterScaling(\n",
       "                (parameter_list_stats): _ParameterListStats(\n",
       "                  (first_tracked_param): _ViewParameterWrapper(\n",
       "                    (view_shape_impl): OverTensorView()\n",
       "                  )\n",
       "                  (stats): _Stats(\n",
       "                    (stats_impl): AbsMax()\n",
       "                  )\n",
       "                )\n",
       "                (stats_scaling_impl): _StatsScaling(\n",
       "                  (affine_rescaling): Identity()\n",
       "                  (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                    (clamp_min_ste): ScalarClampMinSte()\n",
       "                    (restrict_value_impl): FloatRestrictValue()\n",
       "                  )\n",
       "                  (restrict_scaling_pre): Identity()\n",
       "                )\n",
       "              )\n",
       "              (int_scaling_impl): IntScaling()\n",
       "              (zero_point_impl): ZeroZeroPoint(\n",
       "                (zero_point): StatelessBuffer()\n",
       "              )\n",
       "              (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "                (bit_width): StatelessBuffer()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (bias_quant): BiasQuantProxyFromInjector(\n",
       "            (_zero_hw_sentinel): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantConv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): QuantConv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (input_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (output_quant): ActQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "        (weight_quant): WeightQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClampSte()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): StatsFromParameterScaling(\n",
       "              (parameter_list_stats): _ParameterListStats(\n",
       "                (first_tracked_param): _ViewParameterWrapper(\n",
       "                  (view_shape_impl): OverTensorView()\n",
       "                )\n",
       "                (stats): _Stats(\n",
       "                  (stats_impl): AbsMax()\n",
       "                )\n",
       "              )\n",
       "              (stats_scaling_impl): _StatsScaling(\n",
       "                (affine_rescaling): Identity()\n",
       "                (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                  (clamp_min_ste): ScalarClampMinSte()\n",
       "                  (restrict_value_impl): FloatRestrictValue()\n",
       "                )\n",
       "                (restrict_scaling_pre): Identity()\n",
       "              )\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bias_quant): BiasQuantProxyFromInjector(\n",
       "          (_zero_hw_sentinel): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): QuantLinear(\n",
       "    in_features=512, out_features=10, bias=True\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Main export pipeline ---\n",
    "# Step 1: Construct model\n",
    "model = ResNet18(num_classes=10)\n",
    "\n",
    "# Step 2: Load weights\n",
    "trained_state_dict = torch.load(\"./models/quentresnet18_weight8.pth\", map_location='cpu')\n",
    "model.load_state_dict(trained_state_dict, strict=False)\n",
    "\n",
    "# Step 3: Remove export_handler from all quant layers\n",
    "remove_export_handlers(model)\n",
    "\n",
    "# Step 4: Prepare for export\n",
    "model.eval()\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a2c96-0d90-444d-ad5d-1453c50d0c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a25838df-bea4-41cd-811d-969bde5096a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum= 0.9)\n",
    "test_losses, train_losses, test_accs, train_accs = [], [], [], []\n",
    "misclassified_imgs, correct_imgs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be0ab2b1-2c77-4a9c-9637-3a678458dfdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f89aded-b64e-4b8b-943f-1ec12b14b47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1255: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1758.)\n",
      "  return super(Tensor, self).rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of     0 : 100 %\n",
      "Accuracy of     1 : 100 %\n",
      "Accuracy of     2 : 75 %\n",
      "Accuracy of     3 : 85 %\n",
      "Accuracy of     4 : 100 %\n",
      "Accuracy of     5 : 100 %\n",
      "Accuracy of     6 : 100 %\n",
      "Accuracy of     7 : 100 %\n",
      "Accuracy of     8 : 100 %\n",
      "Accuracy of     9 : 66 %\n",
      "\n",
      "Test set: Average loss: 0.3311, Accuracy: 9024/10000 (90.24%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test for accuracy\n",
    "exact_acc = test(model, device, testloader, criterion, classes, test_losses,\n",
    "                 test_accs, misclassified_imgs, correct_imgs,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a66a97-8a00-4684-af36-328decb9f3f5",
   "metadata": {},
   "source": [
    "# 2. Export to FINN-ONNX\n",
    "\n",
    "ONNX is an open format built to represent machine learning models, and the FINN compiler expects an ONNX model as input. We'll now export our network into ONNX to be imported and used in FINN for the next notebooks. Note that the particular ONNX representation used for FINN differs from standard ONNX, you can read more about this here.\n",
    "\n",
    "You can see below how we export a trained network in Brevitas into a FINN-compatible ONNX representation. Note how we create a QuantTensor instance with dummy data to tell Brevitas how our inputs look like, which will be used to set the input quantization annotation on the exported model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d71e9bcc-2153-447a-8643-838f01f5da09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported to hardware/quantresnet18_weight8_files/quantresnet18_raw.onnx\n"
     ]
    }
   ],
   "source": [
    "from brevitas.export import export_brevitas_onnx\n",
    "# Step 5: Export ONNX\n",
    "onnx_export_path = \"hardware/quantresnet18_weight8_files/quantresnet18_raw.onnx\"\n",
    "# Step 5: Export ONNX (keep initializers as inputs to keep weights in graph)\n",
    "export_brevitas_onnx(\n",
    "    model,\n",
    "    input_t=torch.randn(1, 3, 32, 32),\n",
    "    export_path=onnx_export_path,\n",
    "    keep_initializers_as_inputs=True  # Changed here\n",
    ")\n",
    "print(f\"✅ Exported to {onnx_export_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96011958-3c51-4bb5-bad9-eb03ee918191",
   "metadata": {},
   "source": [
    "## Compare FINN & Brevitas execution\n",
    "\n",
    "Load brevitas model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee10b0ef-220f-48e3-9461-117615697e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brevitas_model =  ResNet18(num_classes=10)\n",
    "trained_state_dict = torch.load(\"./models/quentresnet18_weight8.pth\", map_location='cpu')\n",
    "brevitas_model.load_state_dict(trained_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51864fec-8967-45ef-8d5c-ae8a6fa0e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, current_inp, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    current_inp = current_inp.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(current_inp)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    \n",
    "        # Convert output to a Python list for further use (e.g., JSON export)\n",
    "        output_list = output.cpu().detach().numpy().tolist()\n",
    "\n",
    "    return output_list, predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4807c6d8-628d-46e0-befc-988579583d6a",
   "metadata": {},
   "source": [
    "Now that we have the model in .onnx format, we can work with it using FINN. To import it into FINN, we'll use the ModelWrapper. It is a wrapper around the ONNX model which provides several helper functions to make it easier to work with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dccc7b2-71a3-4937-8e41-deef825283dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from brevitas.nn import QuantConv2d, QuantLinear \n",
    "from brevitas.export import export_brevitas_onnx \n",
    "from qonnx.core.modelwrapper import ModelWrapper \n",
    "from qonnx.transformation.fold_constants import FoldConstants \n",
    "from qonnx.transformation.infer_shapes import InferShapes \n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes \n",
    "from qonnx.transformation.general import ( GiveUniqueNodeNames, GiveReadableTensorNames, RemoveStaticGraphInputs ) \n",
    "from finn.transformation.streamline import Streamline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89f49774-459e-4a25-b43a-3c0780152b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned model saved to hardware/quantresnet18_weight8_files/quantresnet18_weight8_tidy.onnx\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import numpy_helper\n",
    "import numpy as np\n",
    "\n",
    "def fold_quant_nodes_producing_weights(model_wrapper):\n",
    "    model = model_wrapper.model\n",
    "    init_map = {init.name: init for init in model.graph.initializer}\n",
    "    nodes_to_remove = []\n",
    "    new_inits = []\n",
    "\n",
    "    for node in model.graph.node:\n",
    "        if node.op_type == \"Quant\":\n",
    "            quant_out = node.output[0]\n",
    "\n",
    "            # Find Conv or Gemm nodes using this quant output as weight input\n",
    "            is_used_as_weight = False\n",
    "            for conv_node in model.graph.node:\n",
    "                if conv_node.op_type in [\"Conv\", \"Gemm\"]:\n",
    "                    if quant_out in conv_node.input:\n",
    "                        is_used_as_weight = True\n",
    "                        break\n",
    "\n",
    "            if not is_used_as_weight:\n",
    "                continue  # skip quant nodes not used as weights\n",
    "\n",
    "            # Check if input to Quant is initializer (unquantized weights)\n",
    "            quant_in_name = node.input[0]\n",
    "            if quant_in_name not in init_map:\n",
    "                print(f\"Input to Quant node '{node.name}' ({quant_in_name}) not found as initializer!\")\n",
    "                continue\n",
    "\n",
    "            # Load weight tensor as numpy\n",
    "            weight_init = init_map[quant_in_name]\n",
    "            weight_np = numpy_helper.to_array(weight_init)\n",
    "\n",
    "            # TODO: Here you need actual quantization logic based on Quant node attributes\n",
    "            # For now, assume quantized weights = original weights (no quantization applied)\n",
    "            quantized_weight_np = weight_np  # Replace with actual quantization logic\n",
    "\n",
    "            # Create new initializer with quant_out name\n",
    "            new_init = numpy_helper.from_array(quantized_weight_np, quant_out)\n",
    "            new_inits.append(new_init)\n",
    "\n",
    "            # Update all Conv/Gemm nodes input from quant_out to use the new initializer (same name)\n",
    "            # No change needed because name is the same, just now it will be a constant initializer\n",
    "\n",
    "            # Mark Quant node for removal\n",
    "            nodes_to_remove.append(node)\n",
    "\n",
    "    # Remove Quant nodes from graph\n",
    "    for node in nodes_to_remove:\n",
    "        model.graph.node.remove(node)\n",
    "\n",
    "    # Add new initializers\n",
    "    model.graph.initializer.extend(new_inits)\n",
    "\n",
    "    # Return updated model wrapper\n",
    "    return ModelWrapper(model)\n",
    "\n",
    "# --- Insert this folding step after your FoldConstants() but before Streamline() ---\n",
    "\n",
    "model_wrapper = ModelWrapper(onnx_export_path)\n",
    "model_wrapper.set_tensor_shape(model_wrapper.graph.input[0].name, [592, 3, 32, 32])\n",
    "model_wrapper = model_wrapper.transform(InferShapes())\n",
    "model_wrapper = model_wrapper.transform(InferDataTypes())\n",
    "model_wrapper = model_wrapper.transform(GiveUniqueNodeNames())\n",
    "model_wrapper = model_wrapper.transform(GiveReadableTensorNames())\n",
    "model_wrapper = model_wrapper.transform(RemoveStaticGraphInputs())\n",
    "model_wrapper = model_wrapper.transform(FoldConstants())\n",
    "\n",
    "# Fold Quant nodes producing weights into initializers\n",
    "model_wrapper = fold_quant_nodes_producing_weights(model_wrapper)\n",
    "\n",
    "# Save cleaned-up model\n",
    "verif_model_filename = \"hardware/quantresnet18_weight8_files/quantresnet18_weight8_tidy.onnx\"\n",
    "model_wrapper.save(verif_model_filename)\n",
    "\n",
    "print(f\"Cleaned model saved to {verif_model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57a53b40-1798-488f-ac47-0b65933e20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "def inference_with_finn_onnx(current_inp: torch.Tensor, model_for_sim):\n",
    "    \"\"\"\n",
    "    Run inference using a FINN-compatible ONNX model.\n",
    "\n",
    "    Args:\n",
    "        current_inp (torch.Tensor): Input tensor of shape (1, C, H, W).\n",
    "        model_for_sim (ModelWrapper): Cleaned and transformed FINN ONNX model.\n",
    "\n",
    "    Returns:\n",
    "        logits (np.ndarray): Raw output from the model.\n",
    "        predicted (np.ndarray): Predicted class index (argmax).\n",
    "    \"\"\"\n",
    "    input_name = model_for_sim.graph.input[0].name\n",
    "    output_name = model_for_sim.graph.output[0].name\n",
    "\n",
    "    # Ensure correct input shape and type\n",
    "    current_inp = current_inp.detach().cpu().numpy()\n",
    "    if current_inp.ndim == 3:\n",
    "        current_inp = np.expand_dims(current_inp, axis=0)  # Add batch dim\n",
    "    current_inp = current_inp.astype(np.float32)\n",
    "\n",
    "    # Build input dict and run inference\n",
    "    input_dict = {input_name: current_inp}\n",
    "    output_dict = oxe.execute_onnx(model_for_sim, input_dict)\n",
    "\n",
    "    # Extract output and predicted class\n",
    "    logits = output_dict[output_name]\n",
    "    predicted = np.argmax(logits, axis=1)\n",
    "\n",
    "    return logits, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "261c00fb-0dae-44bf-b137-df78ada1e2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 10064\n",
      "Total incorrect: 0\n",
      "Agreement Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "ok = 0\n",
    "nok = 0\n",
    "expected_batch_size = 592\n",
    "\n",
    "for img, label_gt in testloader:\n",
    "    img, label_gt = img.to(device), label_gt.to(device)\n",
    "\n",
    "    if img.shape[0] == 0:\n",
    "        continue  # skip empty batches\n",
    "\n",
    "    original_batch_size = img.shape[0]\n",
    "\n",
    "    # Pad or truncate to expected batch size\n",
    "    if original_batch_size < expected_batch_size:\n",
    "        padding_size = expected_batch_size - original_batch_size\n",
    "        img = torch.cat([img, img[:padding_size]], dim=0)\n",
    "        label_gt = torch.cat([label_gt, label_gt[:padding_size]], dim=0)\n",
    "    elif original_batch_size > expected_batch_size:\n",
    "        img = img[:expected_batch_size]\n",
    "        label_gt = label_gt[:expected_batch_size]\n",
    "\n",
    "    # Brevitas inference\n",
    "    _, predicted = inference(brevitas_model, img, device=device)\n",
    "\n",
    "    # FINN ONNX inference\n",
    "    _, predicted_onnx = inference_with_finn_onnx(img, model_wrapper)\n",
    "    predicted_onnx_tensor = torch.tensor(predicted_onnx, dtype=torch.long, device=device)\n",
    "\n",
    "    # Compare predictions between Brevitas and FINN\n",
    "    correct = predicted.eq(predicted_onnx_tensor).sum().item()\n",
    "    incorrect = expected_batch_size - correct\n",
    "\n",
    "    ok += correct\n",
    "    nok += incorrect\n",
    "\n",
    "# Final results\n",
    "print(f\"Total correct: {ok}\")\n",
    "print(f\"Total incorrect: {nok}\")\n",
    "print(f\"Agreement Accuracy: {100. * ok / (ok + nok):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b724d-690a-4b1e-a008-2cb3c5d1ed00",
   "metadata": {},
   "source": [
    "## Adding Pre- and Postprocessing <a id='prepost'></a>\n",
    "\n",
    "In many cases, it's common to apply some preprocessing to the raw data in a machine learning framework prior to training. For image classification networks, this may include conversion of raw 8-bit RGB values into floating point values between 0 and 1. Similarly, at the output of the network some postprocessing may be performed during deployment, such as extracting the indices of the classifications with the largest value (top-K indices).\n",
    "\n",
    "In FINN, we can bake some of these pre/postprocessing operatings into the graph, and in some cases these can be highly beneficial for performance by allowing our accelerator to directly consume raw data instead of going through CPU preprocessing. \n",
    "\n",
    "We'll demonstrate this for our small image classification network as follows. Brevitas preprocesses BNN-PYNQ network inputs with `torchvision.transforms.ToTensor()` [prior to training](https://github.com/Xilinx/brevitas/blob/master/src/brevitas_examples/bnn_pynq/trainer.py#L93), which converts 8-bit RGB values into floats between 0 and 1 by dividing the input by 255. We can achieve the same effect in FINN by exporting a single-node ONNX graph for division by 255 (which already exists as `finn.util.pytorch.ToTensor` and merging this with our original model. Finally, we're going to mark our input tensor as 8-bit to let FINN know which level of precision to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ac14054-c151-4f22-9b56-b197fc8bdb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ToTensorWrapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: UINT8 NHWC (N, H, W, C)\n",
    "        x = x.permute(0, 3, 1, 2).float() / 255.0  # NHWC -> NCHW, normalize\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b65d0df5-0515-404d-abbf-9e41354dec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "# Updated dummy input with correct batch size\n",
    "dummy_input = torch.randint(0, 256, (592, 32, 32, 3), dtype=torch.uint8)\n",
    "\n",
    "# Export preprocessing model again\n",
    "model = ToTensorWrapper()\n",
    "preproc_path = \"hardware/quantresnet18_weight8_files/quantresnet18_weight8_a1_with_preproc.onnx\"\n",
    "export_qonnx(model, dummy_input, preproc_path)\n",
    "qonnx_cleanup(preproc_path, out_file=preproc_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "214f22ef-56e7-41b6-a922-b5f8b75ccb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pamela/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "# Load preprocessing model and convert\n",
    "pre_model = ModelWrapper(preproc_path)\n",
    "pre_model = pre_model.transform(ConvertQONNXtoFINN())\n",
    "\n",
    "# Load core model\n",
    "core_model = ModelWrapper(\"hardware/quantresnet18_weight8_files/quantresnet18_weight8_tidy.onnx\")\n",
    "\n",
    "# Merge\n",
    "core_model = core_model.transform(MergeONNXModels(pre_model))\n",
    "\n",
    "# Set proper input shape and type\n",
    "core_model.set_tensor_shape(core_model.graph.input[0].name, [592, 32, 32, 3])\n",
    "core_model.set_tensor_datatype(core_model.graph.input[0].name, DataType[\"UINT8\"])\n",
    "\n",
    "# Save final model\n",
    "core_model.save(\"hardware/quantresnet18_weight8_files/quantresnet18_weight8_final.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06d267d5-d29a-4dd1-859d-a7bd84da904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core model input shape: [592, 32, 32, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Core model input shape:\", core_model.get_tensor_shape(core_model.graph.input[0].name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3145538d-fba3-481e-b235-f93f78f2ca0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "\n",
    "# Load CIFAR10 sample\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts to [0,1] float\n",
    "    transforms.ConvertImageDtype(torch.uint8)  # Simulate raw input as uint8\n",
    "])\n",
    "dataset = datasets.CIFAR10(root=\".\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1045c312-9cdf-4ea3-b343-59d8508f4f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (592, 10)\n",
      "First 10 predictions: [[-0.3501871  -4.841082   -0.6999654   3.8095608   3.0655427   3.1280637\n",
      "  -0.04158868 -1.3247869  -1.5421141   0.47182822]\n",
      " [ 3.2556887  -1.0761231  -0.02893056  2.819892   -1.1279145  -1.8868216\n",
      "   0.21485473 -3.3927462   2.8908088   0.06721953]\n",
      " [-1.1760231  -6.011578    1.3738385   2.233866    4.4420485   2.1159492\n",
      "   4.670891   -3.52532    -0.63752943 -2.1631691 ]\n",
      " [ 2.5881476  -6.219797    5.804773    4.8108077  -1.0688319   1.214709\n",
      "   2.813208   -2.8741474  -1.1683418  -4.0391526 ]\n",
      " [ 1.1528944  -6.3383408  -0.11458682  4.0101094   1.5729      3.8940346\n",
      "   4.177797   -2.6356783  -0.8261254  -3.0152466 ]\n",
      " [ 1.7585313  -3.8780274  -1.0171657   1.2420057  -1.336038    3.5041962\n",
      "   5.716073   -4.68685     2.6686285  -2.3892045 ]\n",
      " [-0.92201155 -3.2746606  -3.262432    7.4282136  -0.65232545  0.49708307\n",
      "   3.0408783  -1.1937056   2.8359857  -2.6004984 ]\n",
      " [ 2.7571666  -5.8486104   0.23284847  2.3388171   1.246488    1.2227845\n",
      "   2.6103067  -4.5763226   3.9824462  -2.803429  ]\n",
      " [ 0.05555701 -5.405089   -3.8439      6.5930305  -0.6104565   5.7318044\n",
      "   2.8519723  -2.2316108  -0.11485533 -0.9974447 ]\n",
      " [ 0.03835302 -4.507544   -1.8006592   5.1774893  -0.72452676  1.6153402\n",
      "   4.5246024  -2.6916492  -0.27194363  0.368665  ]]\n"
     ]
    }
   ],
   "source": [
    "# Create batch of 592 uint8 CIFAR images\n",
    "imgs = []\n",
    "labels = []\n",
    "for i in range(592):\n",
    "    img, label = dataset[i]\n",
    "    img_np = (img.numpy() * 255).astype(np.uint8)\n",
    "    imgs.append(img_np)\n",
    "    labels.append(label)\n",
    "\n",
    "# Stack and reshape\n",
    "batch_np = np.stack(imgs, axis=0)  # Still (592, 3, 32, 32)\n",
    "batch_np = np.transpose(batch_np, (0, 2, 3, 1))  # Now (592, 32, 32, 3)\n",
    "input_dict = {core_model.graph.input[0].name: batch_np.astype(np.uint8)}\n",
    "\n",
    "# Run inference\n",
    "output_dict = execute_onnx(core_model, input_dict)\n",
    "predictions = output_dict[core_model.graph.output[0].name]\n",
    "\n",
    "print(\"Predictions shape:\", predictions.shape)\n",
    "print(\"First 10 predictions:\", predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c59d7-107f-4fb9-b82c-8fdc7734b276",
   "metadata": {},
   "source": [
    "You can observe two changes in the graph above: a Div node has appeared in the beginning to perform the input preprocessing, and the global_in tensor now has a quantization annotation to mark it as an unsigned 8-bit value.\n",
    "\n",
    "For the postprocessing we'll insert a TopK node for k=1 at the end of our graph. This will extract the index (class number) for the largest-valued output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7dcd9000-cf83-4991-845a-04b112702550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model = core_model.transform(InsertTopK(k=1))\n",
    "chkpt_name = \"hardware/quantresnet18_weight8_files/quantresnet18_weight8_final.onnx\"\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "\n",
    "model.save(chkpt_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28b2a799-655c-44c5-a769-79c7ea31a1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 shape: (592, 1)\n",
      "Top-1 predictions (first 10): [[3]\n",
      " [0]\n",
      " [6]\n",
      " [2]\n",
      " [6]\n",
      " [6]\n",
      " [3]\n",
      " [8]\n",
      " [3]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "input_dict = {model.graph.input[0].name: batch_np.astype(np.uint8)}  # use uint8 now\n",
    "output_dict = execute_onnx(model, input_dict)\n",
    "top1_predictions = output_dict[model.graph.output[0].name]\n",
    "\n",
    "print(\"Top-1 shape:\", top1_predictions.shape)\n",
    "print(\"Top-1 predictions (first 10):\", top1_predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e285bb4-70c1-45b1-a6c2-b6412009e595",
   "metadata": {},
   "source": [
    "## Streamlining <a id='streamline'></a>\n",
    "Streamlining is a transformation containing several sub-transformations. The goal of streamlining is to eliminate floating point operations by moving them around, then collapsing them into one operation and in the last step transform them into multi-thresholding nodes. For more information on the theoretical background of this, see [this paper](https://arxiv.org/pdf/1709.04060).\n",
    "\n",
    "Let's have a look at which sub-transformations `Streamline` consists of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5c5e79d-ab04-449f-aea0-3e55b974141e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved streamlined model to hardware/quantresnet18_weight8_files/quantresnet18_streamlined.onnx\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.streamline.reorder import MoveScalarLinearPastInvariants\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "# Now you can streamline safely\n",
    "model = ModelWrapper(\"hardware/quantresnet18_weight8_files/quantresnet18_weight8_final.onnx\")\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "# Save final model\n",
    "onnx_final_path = \"hardware/quantresnet18_weight8_files/quantresnet18_streamlined.onnx\"\n",
    "model.save(onnx_final_path)\n",
    "print(f\"✅ Saved streamlined model to {onnx_final_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff63c6-65e9-4bb4-bbb0-16922c449827",
   "metadata": {},
   "source": [
    "You can see that the network has become simplified considerably compared to the previous step -- a lot of nodes have disappeared between the `MatMul` layers. \n",
    "\n",
    "**The current implementation of streamlining is highly network-specific and may not work for your network if its topology is very different than the example network here. We hope to rectify this in future releases.**\n",
    "\n",
    "Our example network is a quantized network with 1-bit bipolar (-1, +1 values) precision, and we want FINN to implement them as XNOR-popcount operations [as described in the original FINN paper](https://arxiv.org/pdf/1612.07119). For this reason, after streamlining, the resulting bipolar matrix multiplications are converted into xnorpopcount operations. This transformation produces operations that are again collapsed and converted into thresholds. This procedure is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5fb3ddb-1f95-46ca-9772-302b3e9523ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "from finn.transformation.streamline.round_thresholds import RoundAndClipThresholds\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(absorb.AbsorbAddIntoMultiThreshold())\n",
    "model = model.transform(absorb.AbsorbMulIntoMultiThreshold())\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(RoundAndClipThresholds())\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "model.save(\"hardware/quantresnet18_weight8_files/quantresnet18_ready_for_hw_conversion.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ddb919-a371-45d2-902a-c18165d601be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66f59c46-0c8b-479f-8aee-ce76a1effb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Transpose', 'GlobalAveragePool', 'Gemm', 'Reshape', 'Mul', 'Dropout', 'Relu', 'Add', 'Conv', 'TopK', 'Cast'}\n"
     ]
    }
   ],
   "source": [
    "model = ModelWrapper(\"hardware/quantresnet18_weight8_files/quantresnet18_ready_for_hw_conversion.onnx\")\n",
    "print(set([node.op_type for node in model.graph.node]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11411c-6127-4cc6-8791-7c19c718b4b1",
   "metadata": {},
   "source": [
    "Observe the pairs of `XnorPopcountmatMul` and `MultiThreshold` layers following each other -- this is the particular pattern that the next step will be looking for in order to convert them to hardware (HW) layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767db86-0a86-4f89-9912-2e8a4534a661",
   "metadata": {},
   "source": [
    "## 3. Conversion to HW layers <a id='hw_layers'></a>\n",
    "Converts the nodes to HW layers, these layers are abstraction layers that do not directly correspond to an HLS or Verilog implementation but they will be converted in either one later in the flow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61297e44-021a-445a-b939-b750d079c035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Transpose', 'GlobalAveragePool', 'Gemm', 'Reshape', 'Mul', 'Dropout', 'Relu', 'Add', 'Conv', 'TopK', 'Cast'}\n"
     ]
    }
   ],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "\n",
    "model = ModelWrapper(\"hardware/quantresnet18_weight8_files/quantresnet18_ready_for_hw_conversion.onnx\")\n",
    "model = model.transform(to_hw.InferBinaryMatrixVectorActivation())\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hw.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hw.InferThresholdingLayer())\n",
    "model.save(\"hardware/quantresnet18_weight8_files/quantresnet18_hw_layers.onnx\")\n",
    "\n",
    "model = ModelWrapper(\"hardware/quantresnet18_weight8_files/quantresnet18_hw_layers.onnx\")\n",
    "print(set([node.op_type for node in model.graph.node]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d54ec46-71e8-49b7-ba47-7e93ce246b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DataLayout', 'DataType', 'InferAddStreamsLayer', 'InferBinaryMatrixVectorActivation', 'InferChannelwiseLinearLayer', 'InferConcatLayer', 'InferConvInpGen', 'InferDataTypes', 'InferDuplicateStreamsLayer', 'InferGlobalAccPoolLayer', 'InferLabelSelectLayer', 'InferLookupLayer', 'InferPool', 'InferQuantizedMatrixVectorActivation', 'InferShapes', 'InferStreamingEltwise', 'InferStreamingMaxPool', 'InferThresholdingLayer', 'InferUpsample', 'InferVectorVectorActivation', 'SortGraph', 'TensorProto', 'Transformation', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'getCustomOp', 'get_by_name', 'helper', 'nchw_to_nhwc', 'np', 'warnings']\n"
     ]
    }
   ],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "print(dir(to_hw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26b12778-4a92-4eb6-adfd-8169078a7018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.convert_to_hw_layers import (\n",
    "    InferBinaryMatrixVectorActivation,\n",
    "    InferQuantizedMatrixVectorActivation,\n",
    "    InferLabelSelectLayer,\n",
    "    InferThresholdingLayer,\n",
    "    InferStreamingMaxPool,\n",
    "    InferGlobalAccPoolLayer,\n",
    "    InferStreamingEltwise,\n",
    "    InferConvInpGen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4514716f-663d-47bf-bbf5-872c64a87a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.convert_to_hw_layers import (\n",
    "    InferBinaryMatrixVectorActivation,\n",
    "    InferQuantizedMatrixVectorActivation,\n",
    "    InferLabelSelectLayer,\n",
    "    InferThresholdingLayer,\n",
    "    InferStreamingMaxPool,\n",
    "    InferGlobalAccPoolLayer,\n",
    "    InferStreamingEltwise,\n",
    "    InferConvInpGen\n",
    ")\n",
    "\n",
    "model = ModelWrapper(\"hardware/quantresnet18_weight8_files/quantresnet18_hw_layers.onnx\")\n",
    "\n",
    "model = model.transform(InferBinaryMatrixVectorActivation())\n",
    "model = model.transform(InferQuantizedMatrixVectorActivation())\n",
    "model = model.transform(InferLabelSelectLayer())\n",
    "model = model.transform(InferThresholdingLayer())\n",
    "model = model.transform(InferStreamingMaxPool())\n",
    "model = model.transform(InferGlobalAccPoolLayer())\n",
    "model = model.transform(InferStreamingEltwise())\n",
    "model = model.transform(InferConvInpGen())\n",
    "\n",
    "model.save(\"hardware/quantresnet18_weight8_files/quantresnet18_hw_layers_updated.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d0af6c-93aa-4574-acaf-d38f477c9a3e",
   "metadata": {},
   "source": [
    "### Creating a Dataflow Partition <a id='dataflow_partition'></a>\n",
    "\n",
    "In the graph above, you can see that there is a mixture of FINN HW layers (`MVAU` and `Thresholding`) with one regular ONNX layers (Reshape). To create a bitstream, FINN needs a model with only HW layers. In order to achieve this, we will use the `CreateDataflowPartition` transformation to create a \"dataflow partition\" in this graph, separating out the HLS layers into another model, and replacing them with a placeholder layer called StreamingDataflowPartition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3dd307bd-5b1c-4f01-9e04-b9a1fc94e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.create_dataflow_partition import CreateDataflowPartition\n",
    "\n",
    "model = ModelWrapper(\"hardware/quantresnet18_weight8_files/quantresnet18_hw_layers_updated.onnx\")\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(\"hardware/quantresnet18_weight8_files/quantresnet18_dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee804b1c-4ac8-4f37-8837-8ee643a4ac46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Transpose', 'GlobalAveragePool', 'Gemm', 'Reshape', 'Mul', 'Dropout', 'Relu', 'Add', 'Conv', 'TopK', 'Cast'}\n"
     ]
    }
   ],
   "source": [
    "print(set([node.op_type for node in parent_model.graph.node]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b1f9c54-fbaf-4428-bde7-db0475385077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No StreamingDataflowPartition node found in the model.\n"
     ]
    }
   ],
   "source": [
    "sdp_nodes = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")\n",
    "if len(sdp_nodes) == 0:\n",
    "    print(\"❌ No StreamingDataflowPartition node found in the model.\")\n",
    "else:\n",
    "    sdp_node = getCustomOp(sdp_nodes[0])\n",
    "    dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "    print(\"✅ Found dataflow model:\", dataflow_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75e3d6b5-c508-464c-9bb7-ea6e9507d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(\"hardware/quantresnet18_weight8_files/quantresnet18_hw_layers_updated.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a1525bc-fbdc-4a91-93ef-987b668c601d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose_0 Transpose\n",
      "Cast_0 Cast\n",
      "Conv_0 Conv\n",
      "Mul_0 Mul\n",
      "Add_0 Add\n",
      "Relu_0 Relu\n",
      "Dropout_0 Dropout\n",
      "Conv_1 Conv\n",
      "Mul_1 Mul\n",
      "Add_1 Add\n",
      "Relu_1 Relu\n",
      "Dropout_1 Dropout\n",
      "Conv_2 Conv\n",
      "Mul_2 Mul\n",
      "Add_2 Add\n",
      "Dropout_2 Dropout\n",
      "Add_3 Add\n",
      "Relu_2 Relu\n",
      "Dropout_3 Dropout\n",
      "Conv_3 Conv\n",
      "Mul_3 Mul\n",
      "Add_4 Add\n",
      "Relu_3 Relu\n",
      "Dropout_4 Dropout\n",
      "Conv_4 Conv\n",
      "Mul_4 Mul\n",
      "Add_5 Add\n",
      "Dropout_5 Dropout\n",
      "Add_6 Add\n",
      "Relu_4 Relu\n",
      "Dropout_6 Dropout\n",
      "Conv_5 Conv\n",
      "Conv_6 Conv\n",
      "Mul_5 Mul\n",
      "Mul_6 Mul\n",
      "Add_7 Add\n",
      "Add_8 Add\n",
      "Relu_5 Relu\n",
      "Dropout_7 Dropout\n",
      "Conv_7 Conv\n",
      "Mul_7 Mul\n",
      "Add_9 Add\n",
      "Dropout_8 Dropout\n",
      "Add_10 Add\n",
      "Relu_6 Relu\n",
      "Dropout_9 Dropout\n",
      "Conv_8 Conv\n",
      "Mul_8 Mul\n",
      "Add_11 Add\n",
      "Relu_7 Relu\n",
      "Dropout_10 Dropout\n",
      "Conv_9 Conv\n",
      "Mul_9 Mul\n",
      "Add_12 Add\n",
      "Dropout_11 Dropout\n",
      "Add_13 Add\n",
      "Relu_8 Relu\n",
      "Dropout_12 Dropout\n",
      "Conv_10 Conv\n",
      "Conv_11 Conv\n",
      "Mul_10 Mul\n",
      "Mul_11 Mul\n",
      "Add_14 Add\n",
      "Add_15 Add\n",
      "Relu_9 Relu\n",
      "Dropout_13 Dropout\n",
      "Conv_12 Conv\n",
      "Mul_12 Mul\n",
      "Add_16 Add\n",
      "Dropout_14 Dropout\n",
      "Add_17 Add\n",
      "Relu_10 Relu\n",
      "Dropout_15 Dropout\n",
      "Conv_13 Conv\n",
      "Mul_13 Mul\n",
      "Add_18 Add\n",
      "Relu_11 Relu\n",
      "Dropout_16 Dropout\n",
      "Conv_14 Conv\n",
      "Mul_14 Mul\n",
      "Add_19 Add\n",
      "Dropout_17 Dropout\n",
      "Add_20 Add\n",
      "Relu_12 Relu\n",
      "Dropout_18 Dropout\n",
      "Conv_15 Conv\n",
      "Conv_16 Conv\n",
      "Mul_15 Mul\n",
      "Mul_16 Mul\n",
      "Add_21 Add\n",
      "Add_22 Add\n",
      "Relu_13 Relu\n",
      "Dropout_19 Dropout\n",
      "Conv_17 Conv\n",
      "Mul_17 Mul\n",
      "Add_23 Add\n",
      "Dropout_20 Dropout\n",
      "Add_24 Add\n",
      "Relu_14 Relu\n",
      "Dropout_21 Dropout\n",
      "Conv_18 Conv\n",
      "Mul_18 Mul\n",
      "Add_25 Add\n",
      "Relu_15 Relu\n",
      "Dropout_22 Dropout\n",
      "Conv_19 Conv\n",
      "Mul_19 Mul\n",
      "Add_26 Add\n",
      "Dropout_23 Dropout\n",
      "Add_27 Add\n",
      "Relu_16 Relu\n",
      "Dropout_24 Dropout\n",
      "GlobalAveragePool_0 GlobalAveragePool\n",
      "Reshape_0 Reshape\n",
      "Gemm_0 Gemm\n",
      "TopK_0 TopK\n"
     ]
    }
   ],
   "source": [
    "for node in model.graph.node:\n",
    "        print(node.name, node.op_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22ce6707-08b7-44c1-9f39-beb9ed56180d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'show_net_structure' from 'finn.util.basic' (/home/pamela/finn/src/finn/util/basic.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfinn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_net_structure\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'show_net_structure' from 'finn.util.basic' (/home/pamela/finn/src/finn/util/basic.py)"
     ]
    }
   ],
   "source": [
    "from finn.util.basic import show_net_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed58e62-bf45-4557-90f7-92bc28c4bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in model.graph.node:\n",
    "    if node.op_type in [\"Conv\",\"Gemm\"]:\n",
    "        print(node.name, node.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c43895fb-d402-4d35-87ab-dc48f8229b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv_0_param1 1\n",
      "Conv_1_param1 1\n",
      "Conv_2_param1 1\n",
      "Conv_3_param1 1\n",
      "Conv_4_param1 1\n",
      "Conv_5_param1 1\n",
      "Conv_7_param1 1\n",
      "Conv_6_param1 1\n",
      "Conv_8_param1 1\n",
      "Conv_9_param1 1\n",
      "Conv_10_param1 1\n",
      "Conv_12_param1 1\n",
      "Conv_11_param1 1\n",
      "Conv_13_param1 1\n",
      "Conv_14_param1 1\n",
      "Conv_15_param1 1\n",
      "Conv_17_param1 1\n",
      "Conv_16_param1 1\n",
      "Conv_18_param1 1\n",
      "Conv_19_param1 1\n",
      "Conv_0_param0 1\n",
      "Conv_1_param0 1\n",
      "Conv_2_param0 1\n",
      "Conv_3_param0 1\n",
      "Conv_4_param0 1\n",
      "Conv_5_param0 1\n",
      "Conv_7_param0 1\n",
      "Conv_6_param0 1\n",
      "Conv_8_param0 1\n",
      "Conv_9_param0 1\n",
      "Conv_10_param0 1\n",
      "Conv_12_param0 1\n",
      "Conv_11_param0 1\n",
      "Conv_13_param0 1\n",
      "Conv_14_param0 1\n",
      "Conv_15_param0 1\n",
      "Conv_17_param0 1\n",
      "Conv_16_param0 1\n",
      "Conv_18_param0 1\n",
      "Conv_19_param0 1\n"
     ]
    }
   ],
   "source": [
    "for init in model.model.graph.initializer:\n",
    "    if \"Conv\" in init.name:\n",
    "        print(init.name, init.data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cf45d7f-e9ff-4cc9-8fb6-d0e60fb03ed4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241m.\u001b[39minput:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inp\u001b[38;5;241m.\u001b[39mname, model\u001b[38;5;241m.\u001b[39mget_tensor_datatype(inp\u001b[38;5;241m.\u001b[39mname))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'graph'"
     ]
    }
   ],
   "source": [
    "for inp in model.graph.input:\n",
    "    print(inp.name, model.get_tensor_datatype(inp.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa7d23-c1de-4636-b77b-c371d459be84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
