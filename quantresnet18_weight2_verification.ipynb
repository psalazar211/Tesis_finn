{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINN - Functional Verification of End-to-End Flow\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "**Important: This notebook depends on the [tfc_end2end_example](tfc_end2end_example.ipynb) notebook, because we are using models that are available at intermediate steps in the end-to-end flow. So please make sure the needed .onnx files are generated to run this notebook.**\n",
    "\n",
    "In this notebook, we will show how to take the intermediate results of the end-to-end tfc example and verify their functionality with different methods. In the following picture you can see the section in the end-to-end flow about the *Simulation & Emulation Flows*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"verification.svg\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following helper functions, `showSrc` to show source code of FINN library calls and `showInNetron` to show the ONNX model at the current transformation step. The Netron displays are interactive, but they only work when running the notebook actively and not on GitHub (i.e. if you are viewing this on GitHub you'll only see blank squares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showSrc, showInNetron\n",
    "import os\n",
    "\n",
    "build_dir = os.environ[\"FINN_BUILD_DIR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the simulations, a \"golden\" output is calculated as a reference. This is calculated directly from the Brevitas model using PyTorch, by running some example data from the MNIST dataset through the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1255: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1758.)\n",
      "  return super(Tensor, self).rename(names)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.4090618, -1.3267527,  0.9779036, -1.2444434, -1.4090618,\n",
       "        -1.6559892, -1.3267527, -1.4913709, -1.3267527, -1.6559892]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pkgutil import get_data\n",
    "import onnx\n",
    "import onnx.numpy_helper as nph\n",
    "import torch\n",
    "from finn.util.test import get_test_model_trained\n",
    "\n",
    "fc = get_test_model_trained(\"TFC\", 1, 1)\n",
    "raw_i = get_data(\"qonnx.data\", \"onnx/mnist-conv/test_data_set_0/input_0.pb\")\n",
    "input_tensor = onnx.load_tensor_from_string(raw_i)\n",
    "input_brevitas = torch.from_numpy(nph.to_array(input_tensor).copy()).float()\n",
    "output_golden = fc.forward(input_brevitas).detach().numpy()\n",
    "output_golden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation using Python <a id='simpy'></a>\n",
    "\n",
    "If an ONNX model consists of [standard ONNX](https://github.com/onnx/onnx/blob/main/docs/Operators.md) nodes and/or FINN custom operations that do not belong to the fpgadataflow (`backend` $\\neq$ `fpgadataflow.hls` or `backend` $\\neq$ `fpgadataflow.rtl`) this model can be checked for functionality using Python.\n",
    "\n",
    "To simulate a standard ONNX node [onnxruntime](https://github.com/microsoft/onnxruntime) is used. onnxruntime is an open source tool developed by Microsoft to run standard ONNX nodes. For the FINN custom op nodes execution, functions are defined. The following is an example of the execution function of an XNOR popcount node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def xnorpopcountmatmul(inp0, inp1):\n",
      "    \"\"\"Simulates XNOR-popcount matrix multiplication as a regular bipolar\n",
      "    matrix multiplication followed by some post processing.\"\"\"\n",
      "    # extract the operand shapes\n",
      "    # (M, K0) = inp0.shape\n",
      "    # (K1, N) = inp1.shape\n",
      "    K0 = inp0.shape[-1]\n",
      "    K1 = inp1.shape[0]\n",
      "    # make sure shapes are compatible with matmul\n",
      "    assert K0 == K1, \"Matrix shapes are not compatible with matmul.\"\n",
      "    K = K0\n",
      "    # convert binary inputs to bipolar\n",
      "    inp0_bipolar = 2.0 * inp0 - 1.0\n",
      "    inp1_bipolar = 2.0 * inp1 - 1.0\n",
      "    # call regular numpy matrix multiplication\n",
      "    out = np.matmul(inp0_bipolar, inp1_bipolar)\n",
      "    # XNOR-popcount does not produce the regular dot product result --\n",
      "    # it returns the number of +1s after XNOR. let P be the number of +1s\n",
      "    # and N be the number of -1s. XNOR-popcount returns P, whereas the\n",
      "    # regular dot product result from numpy is P-N, so we need to apply\n",
      "    # some correction.\n",
      "    # out = P-N\n",
      "    # K = P+N\n",
      "    # out + K = 2P, so P = (out + K)/2\n",
      "    return (out + K) * 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from qonnx.custom_op.general.xnorpopcount import xnorpopcountmatmul\n",
    "showSrc(xnorpopcountmatmul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function contains a description of the behaviour in Python and can thus calculate the result of the node.\n",
    "\n",
    "This execution function and onnxruntime is used when `execute_onnx` from `onnx_exec` is applied to the model. The model is then simulated node by node and the result is stored in a context dictionary, which contains the values of each tensor at the end of the execution. To get the result, only the output tensor has to be extracted.\n",
    "\n",
    "The procedure is shown below. We take the model right before the nodes should be converted into HW layers and generate an input tensor to pass to the execution function. The input tensor is generated from the Brevitas example inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "class GetTransforms():\n",
    "    '''Returns a list of transformations when type as requested amongst train/test\n",
    "       Transforms('train') = list of transforms to apply on training data\n",
    "       Transforms('test') = list of transforms to apply on testing data'''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def trainparams(self):\n",
    "        train_transformations = [ #resises the image so it can be perfect for our model.\n",
    "            transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
    "            transforms.RandomRotation((-7,7)),     #Rotates the image to a specified angel\n",
    "            transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n",
    "            transforms.ToTensor(), # comvert the image to tensor so that it can work with torch\n",
    "            transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261)) #Normalize all the images\n",
    "            ]\n",
    "\n",
    "        return train_transformations\n",
    "\n",
    "    def testparams(self):\n",
    "        test_transforms = [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261))\n",
    "        ]\n",
    "        return test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = GetTransforms()\n",
    "train_transforms = transforms.Compose(transformations.trainparams())\n",
    "test_transforms = transforms.Compose(transformations.testparams())\n",
    "\n",
    "\n",
    "class GetCIFAR10_TrainData():\n",
    "    def __init__(self, dir_name:str):\n",
    "        self.dirname = dir_name\n",
    "\n",
    "    def download_train_data(self):\n",
    "        return datasets.CIFAR10('resnet18/data', train=True, download=True, transform=train_transforms)\n",
    "\n",
    "    def download_test_data(self):\n",
    "        return datasets.CIFAR10('resnet18/data', train=False, download=True, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data = GetCIFAR10_TrainData(os.chdir(\"..\"))\n",
    "trainset = data.download_train_data()\n",
    "testset = data.download_test_data()\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=592,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=592,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "model_for_sim = ModelWrapper(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_for_hw_conversion.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "def inference_with_finn_onnx(current_inp: torch.Tensor, model_for_sim):\n",
    "    \"\"\"\n",
    "    Run inference using a FINN-compatible ONNX model.\n",
    "\n",
    "    Args:\n",
    "        current_inp (torch.Tensor): Input tensor of shape (1, C, H, W).\n",
    "        model_for_sim (ModelWrapper): Cleaned and transformed FINN ONNX model.\n",
    "\n",
    "    Returns:\n",
    "        logits (np.ndarray): Raw output from the model.\n",
    "        predicted (np.ndarray): Predicted class index (argmax).\n",
    "    \"\"\"\n",
    "    input_name = model_for_sim.graph.input[0].name\n",
    "    output_name = model_for_sim.graph.output[0].name\n",
    "\n",
    "    # Ensure correct input shape and type\n",
    "    current_inp = current_inp.detach().cpu().numpy()\n",
    "    if current_inp.ndim == 3:\n",
    "        current_inp = np.expand_dims(current_inp, axis=0)  # Add batch dim\n",
    "    current_inp = current_inp.astype(np.float32)\n",
    "\n",
    "    # Build input dict and run inference\n",
    "    input_dict = {input_name: current_inp}\n",
    "    output_dict = oxe.execute_onnx(model_for_sim, input_dict)\n",
    "\n",
    "    # Extract output and predicted class\n",
    "    logits = output_dict[output_name]\n",
    "    predicted = np.argmax(logits, axis=1)\n",
    "\n",
    "    return logits, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Shape mismatch for provided input global_in: found (1, 3, 32, 32) expected (592, 3, 32, 32) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m     label_gt \u001b[38;5;241m=\u001b[39m label_gt[:expected_batch_size]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# FINN ONNX inference\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m _, predicted_onnx \u001b[38;5;241m=\u001b[39m \u001b[43minference_with_finn_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_for_sim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m predicted_onnx_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(predicted_onnx, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[23], line 28\u001b[0m, in \u001b[0;36minference_with_finn_onnx\u001b[0;34m(current_inp, model_for_sim)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Build input dict and run inference\u001b[39;00m\n\u001b[1;32m     27\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m {input_name: current_inp}\n\u001b[0;32m---> 28\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[43moxe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_for_sim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Extract output and predicted class\u001b[39;00m\n\u001b[1;32m     31\u001b[0m logits \u001b[38;5;241m=\u001b[39m output_dict[output_name]\n",
      "File \u001b[0;32m/home/pamela/finn/src/finn/core/onnx_exec.py:54\u001b[0m, in \u001b[0;36mexecute_onnx\u001b[0;34m(model, input_dict, return_full_exec_context, start_node, end_node)\u001b[0m\n\u001b[1;32m     52\u001b[0m model_exec_mode \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_metadata_prop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexec_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (model_exec_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (model_exec_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexecute_onnx_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_full_exec_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mcheck_all_tensor_shapes_specified():\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound unspecified tensor shapes, try infer_shapes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/pamela/finn/deps/qonnx/src/qonnx/core/onnx_exec.py:145\u001b[0m, in \u001b[0;36mexecute_onnx\u001b[0;34m(model, input_dict, return_full_exec_context, start_node, end_node)\u001b[0m\n\u001b[1;32m    143\u001b[0m             execution_context[inp_name] \u001b[38;5;241m=\u001b[39m input_dict[inp_name]\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    146\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch for provided input \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: found \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m                 \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    148\u001b[0m                     inp_name,\n\u001b[1;32m    149\u001b[0m                     \u001b[38;5;28mstr\u001b[39m(execution_context[inp_name]\u001b[38;5;241m.\u001b[39mshape),\n\u001b[1;32m    150\u001b[0m                     \u001b[38;5;28mstr\u001b[39m(input_dict[inp_name]\u001b[38;5;241m.\u001b[39mshape),\n\u001b[1;32m    151\u001b[0m                 )\n\u001b[1;32m    152\u001b[0m             )\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# raise Exception(\"Provided input not found in graph context: %s\" % inp_name)\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# check if model has an execution mode set\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# if None, execute model node by node using execute_node()\u001b[39;00m\n\u001b[1;32m    158\u001b[0m model_exec_mode \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_metadata_prop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexec_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Shape mismatch for provided input global_in: found (1, 3, 32, 32) expected (592, 3, 32, 32) "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "ok = 0\n",
    "nok = 0\n",
    "expected_batch_size = 592\n",
    "\n",
    "for img, label_gt in testloader:\n",
    "    img, label_gt = img.to(device), label_gt.to(device)\n",
    "   \n",
    "    if img.shape[0] == 0:\n",
    "        continue  # skip empty batches\n",
    "\n",
    "    original_batch_size = img.shape[0]\n",
    "\n",
    "    # Pad or truncate to expected batch size\n",
    "    if original_batch_size < expected_batch_size:\n",
    "        padding_size = expected_batch_size - original_batch_size\n",
    "        img = torch.cat([img, img[:padding_size]], dim=0)\n",
    "        label_gt = torch.cat([label_gt, label_gt[:padding_size]], dim=0)\n",
    "    elif original_batch_size > expected_batch_size:\n",
    "        img = img[:expected_batch_size]\n",
    "        label_gt = label_gt[:expected_batch_size]\n",
    "\n",
    "  \n",
    "    # FINN ONNX inference\n",
    "    _, predicted_onnx = inference_with_finn_onnx(img, model_for_sim)\n",
    "    predicted_onnx_tensor = torch.tensor(predicted_onnx, dtype=torch.long, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unsupported dtype dtype('float64') for randint",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfinn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx_exec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moxe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_in\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m      3\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m oxe\u001b[38;5;241m.\u001b[39mexecute_onnx(model_for_sim, input_dict, return_full_exec_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m output_pysim \u001b[38;5;241m=\u001b[39m output_dict[\u001b[38;5;28mlist\u001b[39m(output_dict\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:798\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unsupported dtype dtype('float64') for randint"
     ]
    }
   ],
   "source": [
    "import finn.core.onnx_exec as oxe\n",
    "input_dict = {\"global_in\": np.random.randint(0, 255, size=(1, 3, 32, 32), dtype=np.float64)}\n",
    "output_dict = oxe.execute_onnx(model_for_sim, input_dict, return_full_exec_context=False)\n",
    "output_pysim = output_dict[list(output_dict.keys())[0]]\n",
    "\n",
    "#try:\n",
    "#    assert np.isclose(output_pysim, np.where(output_golden[0]==np.amax(output_golden[0])), atol=1e-3).all()\n",
    "#    print(\"Results are the same!\")\n",
    "#except AssertionError:\n",
    "#    assert False, \"The results are not the same!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is compared with the theoretical \"golden\" value for verification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation (cppsim) using C++\n",
    "\n",
    "When dealing with HLS or RTL custom op nodes in FINN the simulation using Python is no longer sufficient. If the nodes are specialized to HLS layers, the simulation using C++ can be used. To do this, the input tensor is stored in a .npy file and C++ code is generated that reads the values from the .npy array, streams them to the corresponding `finn-hlslib` function and writes the result to a new .npy file. This in turn can be read in Python and processed in the FINN flow. For this example the model after setting the folding factors in the HLS variants of the layers, please be aware that this is not the full model, but the dataflow partition, so before executing at the end of this section we have to integrate the model back into the parent model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note: HW layer can also be converted to RTL variants, in this case \"cppsim\" is not an option we can execute. If nevertheless \"cppsim\" is selected as execution mode for the layer, the execution defaults to the parent class. Like this, networks with a mix of HLS and RTL layers can be executed using \"cppsim\" for the HLS layers.</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_cppsim = ModelWrapper(build_dir+\"/tfc_w1_a1_set_folding_factors.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the code for this simulation and to generate the executable two transformations are used:\n",
    "* `PrepareCppSim` which generates the C++ code for the corresponding HLS layer\n",
    "* `CompileCppSim` which compules the C++ code and stores the path to the executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.prepare_cppsim import PrepareCppSim\n",
    "from finn.transformation.fpgadataflow.compile_cppsim import CompileCppSim\n",
    "from qonnx.transformation.general import GiveUniqueNodeNames\n",
    "\n",
    "model_for_cppsim = model_for_cppsim.transform(GiveUniqueNodeNames())\n",
    "model_for_cppsim = model_for_cppsim.transform(PrepareCppSim())\n",
    "model_for_cppsim = model_for_cppsim.transform(CompileCppSim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we take a look at the model using netron, we can see that the transformations introduced new attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_cppsim.save(build_dir+\"/tfc_w1_a1_for_cppsim.onnx\")\n",
    "#showInNetron(build_dir+\"/tfc_w1_a1_for_cppsim.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following node attributes have been added:\n",
    "* `code_gen_dir_cppsim` indicates the directory where the files for the simulation using C++ are stored\n",
    "* `executable_path` specifies the path to the executable\n",
    "\n",
    "We take now a closer look into the files that were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.custom_op.registry import getCustomOp\n",
    "\n",
    "fc0 = model_for_cppsim.graph.node[1]\n",
    "fc0w = getCustomOp(fc0)\n",
    "code_gen_dir = fc0w.get_nodeattr(\"code_gen_dir_cppsim\")\n",
    "!ls {code_gen_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the .cpp file, the folder contains .h files with the weights and thresholds. The shell script contains the compile command and *node_model* is the executable generated by compilation. Comparing this with the `executable_path` node attribute, it can be seen that it specifies exactly the path to *node_model*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the model the execution mode(exec_mode) must be set to \"cppsim\". This is done using the transformation SetExecMode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.set_exec_mode import SetExecMode\n",
    "\n",
    "model_for_cppsim = model_for_cppsim.transform(SetExecMode(\"cppsim\"))\n",
    "model_for_cppsim.save(build_dir+\"/tfc_w1_a1_for_cppsim.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the model can be executed using `execute_onnx`, we integrate the child model in the parent model. The function reads then the `exec_mode` and writes the input into the correct directory in a .npy file. To be able to read this in C++, there is an additional .hpp file ([npy2apintstream.hpp](https://github.com/Xilinx/finn/blob/main/src/finn/qnn-data/cpp/npy2apintstream.hpp)) in FINN, which uses cnpy to read .npy files and convert them into streams, or to read a stream and write it into an .npy. [cnpy](https://github.com/rogersce/cnpy) is a helper to read and write .npy and .npz formates in C++.\n",
    "\n",
    "The result is again compared to the \"golden\" output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_model = ModelWrapper(build_dir+\"/tfc_w1_a1_dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.graph.node[1]\n",
    "child_model = build_dir + \"/tfc_w1_a1_for_cppsim.onnx\"\n",
    "getCustomOp(sdp_node).set_nodeattr(\"model\", child_model)\n",
    "output_dict = oxe.execute_onnx(parent_model, input_dict)\n",
    "output_cppsim = output_dict[list(output_dict.keys())[0]]\n",
    "\n",
    "try:\n",
    "    assert np.isclose(output_cppsim, np.where(output_golden[0]==np.amax(output_golden[0])), atol=1e-3).all()\n",
    "    print(\"Results are the same!\")\n",
    "except AssertionError:\n",
    "    assert False, \"The results are not the same!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emulation (rtlsim) using PyVerilator\n",
    "\n",
    "The emulation using [PyVerilator](https://github.com/maltanar/pyverilator) can be done after IP blocks are generated from the corresponding HLS layers or for RTL layers directly using the generated Verilog files. Pyverilator is a tool which makes it possible to simulate verilog files using verilator via a python interface.\n",
    "\n",
    "We have two ways to use rtlsim, one is to run the model node-by-node as with the simulation methods, but if the model is in the form of the dataflow partition, the part of the graph that consist of only HLS/RTL nodes could also be executed as whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because at the point where we want to grab and verify the model, the model is already in split form (parent graph consisting of non-hls layers and child graph consisting only of hls layers) we first have to reference the child graph within the parent graph. This is done using the node attribute `model` for the `StreamingDataflowPartition` node.\n",
    "\n",
    "First the procedure is shown, if the child graph has ip blocks corresponding to the individual layers, then the procedure is shown, if the child graph already has a stitched IP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emulation of model node-by-node\n",
    "\n",
    "The child model is loaded and the `exec_mode` for each node is set. To prepare the node-by-node emulation the transformation `PrepareRTLSim` is applied to the child model. With this transformation the emulation files are created for each node and can be used directly when calling `execute_onnx()`. Each node has a new node attribute \"rtlsim_so\" after transformation, which contains the path to the corresponding emulation files. Then it is saved in a new .onnx file so that the changed model can be referenced in the parent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.prepare_rtlsim import PrepareRTLSim\n",
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "\n",
    "test_fpga_part = \"xc7z020clg400-1\"\n",
    "target_clk_ns = 10\n",
    "\n",
    "child_model = ModelWrapper(build_dir + \"/tfc_w1_a1_set_folding_factors.onnx\")\n",
    "child_model = child_model.transform(GiveUniqueNodeNames())\n",
    "child_model = child_model.transform(PrepareIP(test_fpga_part, target_clk_ns))\n",
    "child_model = child_model.transform(HLSSynthIP())\n",
    "child_model = child_model.transform(SetExecMode(\"rtlsim\"))\n",
    "child_model = child_model.transform(PrepareRTLSim())\n",
    "child_model.save(build_dir + \"/tfc_w1_a1_dataflow_child.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load the parent model and set the node attribute `model` in the StreamingDataflowPartition node (`sdp_node`). Afterwards the `exec_mode` is set in the parent model in each node and the model can be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent model\n",
    "model_for_rtlsim = ModelWrapper(build_dir + \"/tfc_w1_a1_dataflow_parent.onnx\")\n",
    "# reference child model\n",
    "sdp_node = getCustomOp(model_for_rtlsim.graph.node[1])\n",
    "sdp_node.set_nodeattr(\"model\", build_dir + \"/tfc_w1_a1_dataflow_child.onnx\")\n",
    "\n",
    "model_for_rtlsim = model_for_rtlsim.transform(SetExecMode(\"rtlsim\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = oxe.execute_onnx(model_for_rtlsim, input_dict)\n",
    "output_rtlsim = output_dict[list(output_dict.keys())[0]]\n",
    "\n",
    "try:\n",
    "    assert np.isclose(output_rtlsim, np.where(output_golden[0]==np.amax(output_golden[0])), atol=1e-3).all()\n",
    "    print(\"Results are the same!\")\n",
    "except AssertionError:\n",
    "    assert False, \"The results are not the same!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emulation of stitched IP\n",
    "\n",
    "Here we use the same procedure. First the child model is loaded, but in contrast to the layer-by-layer emulation, the metadata property `exec_mode` is set to \"rtlsim\" for the whole child model. When the model is integrated and executed in the last step, the verilog files of the stitched IP of the child model are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "\n",
    "child_model = ModelWrapper(build_dir + \"/tfc_w1_a1_dataflow_child.onnx\")\n",
    "child_model = child_model.transform(InsertDWC())  \n",
    "child_model = child_model.transform(InsertFIFO(create_shallow_fifos=True))\n",
    "# DWC and FIFOs need to be specialized to either HLS or RTL variants\n",
    "child_model = child_model.transform(SpecializeLayers(test_fpga_part))\n",
    "child_model.save(build_dir + \"/test.onnx\");\n",
    "child_model = child_model.transform(GiveUniqueNodeNames())\n",
    "child_model = child_model.transform(PrepareIP(test_fpga_part, target_clk_ns))\n",
    "child_model = child_model.transform(HLSSynthIP())\n",
    "child_model = child_model.transform(CreateStitchedIP(test_fpga_part, target_clk_ns))\n",
    "child_model = child_model.transform(PrepareRTLSim())\n",
    "child_model.set_metadata_prop(\"exec_mode\",\"rtlsim\")\n",
    "child_model.save(build_dir + \"/tfc_w1_a1_dataflow_child.onnx\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent model\n",
    "model_for_rtlsim = ModelWrapper(build_dir + \"/tfc_w1_a1_dataflow_parent.onnx\")\n",
    "# reference child model\n",
    "sdp_node = getCustomOp(model_for_rtlsim.graph.node[1])\n",
    "sdp_node.set_nodeattr(\"model\", build_dir + \"/tfc_w1_a1_dataflow_child.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = oxe.execute_onnx(model_for_rtlsim, input_dict)\n",
    "output_rtlsim = output_dict[list(output_dict.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert np.isclose(output_rtlsim, np.where(output_golden[0]==np.amax(output_golden[0])), atol=1e-3).all()\n",
    "    print(\"Results are the same!\")\n",
    "except AssertionError:\n",
    "    assert False, \"The results are not the same!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
