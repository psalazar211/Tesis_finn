{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1be6086-e5c8-44c7-8cb1-1b9086ca1bb3",
   "metadata": {},
   "source": [
    "# Train a Resnet on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95c350c-23be-4021-bd56-ee667fa25aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: netron in /tmp/home_dir/.local/lib/python3.10/site-packages (8.6.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbec4c63-6b55-4bd2-b8df-81e9d6309248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19547837-fba0-4f0d-a72c-8bb4c5b6394f",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3283e32a-701d-41e4-b374-ea988eee8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "class GetTransforms():\n",
    "    '''Returns a list of transformations when type as requested amongst train/test\n",
    "       Transforms('train') = list of transforms to apply on training data\n",
    "       Transforms('test') = list of transforms to apply on testing data'''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def trainparams(self):\n",
    "        train_transformations = [ #resises the image so it can be perfect for our model.\n",
    "            transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
    "            transforms.RandomRotation((-7,7)),     #Rotates the image to a specified angel\n",
    "            transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n",
    "            transforms.ToTensor(), # comvert the image to tensor so that it can work with torch\n",
    "            transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261)) #Normalize all the images\n",
    "            ]\n",
    "\n",
    "        return train_transformations\n",
    "\n",
    "    def testparams(self):\n",
    "        test_transforms = [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261))\n",
    "        ]\n",
    "        return test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19784ea3-806e-410e-a8ca-cb07eb84d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = GetTransforms()\n",
    "train_transforms = transforms.Compose(transformations.trainparams())\n",
    "test_transforms = transforms.Compose(transformations.testparams())\n",
    "\n",
    "\n",
    "class GetCIFAR10_TrainData():\n",
    "    def __init__(self, dir_name:str):\n",
    "        self.dirname = dir_name\n",
    "\n",
    "    def download_train_data(self):\n",
    "        return datasets.CIFAR10('resnet18/data', train=True, download=True, transform=train_transforms)\n",
    "\n",
    "    def download_test_data(self):\n",
    "        return datasets.CIFAR10('resnet18/data', train=False, download=True, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0eabbf4-4d30-4b6d-bd3e-d61f8ef310a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data = GetCIFAR10_TrainData(os.chdir(\"..\"))\n",
    "trainset = data.download_train_data()\n",
    "testset = data.download_test_data()\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=592,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=592,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d951b6a-10dd-4f9d-a68f-e7920c0cf397",
   "metadata": {},
   "source": [
    "\n",
    "# Define a PyTorch Device\n",
    "\n",
    "GPUs can significantly speed-up training of deep neural networks. We check for availability of a GPU and if so define it as target device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8fb4746-a37c-4808-99cf-079cedebdf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763818ac-8946-44b5-9513-f109b57607b1",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b36ede76-2f8f-41b6-b3b7-6fddfa9ea560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model define\n"
     ]
    }
   ],
   "source": [
    "from brevitas.nn import QuantConv2d, QuantLinear, QuantReLU\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "weight_bit_width = 2\n",
    "act_bit_width = 2\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, dropout=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = QuantConv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = QuantReLU(bit_width=act_bit_width, quant_type=\"int\")\n",
    "        self.conv2 = QuantConv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = QuantReLU(bit_width=act_bit_width, quant_type=\"int\")\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                QuantConv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\"),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = F.dropout(out, p=self.dropout)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = F.dropout(out, p=self.dropout)\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu2(out)\n",
    "        out = F.dropout(out, p=self.dropout)\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = QuantConv2d(in_planes, planes, kernel_size=1, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = QuantReLU(bit_width=act_bit_width, quant_type=\"int\")\n",
    "        self.conv2 = QuantConv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = QuantReLU(bit_width=act_bit_width, quant_type=\"int\")\n",
    "        self.conv3 = QuantConv2d(planes, self.expansion*planes, kernel_size=1, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "        self.relu3 = QuantReLU(bit_width=act_bit_width, quant_type=\"int\")\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                QuantConv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\"),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.relu2(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu3(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=200, dropout=0.0):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.conv1 = QuantConv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = QuantReLU(bit_width=act_bit_width, quant_type=\"int\")\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = QuantLinear(512*block.expansion, num_classes, bias=True, weight_bit_width=weight_bit_width,quant_type=\"int\")\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, dropout=self.dropout))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = F.dropout(out, p=self.dropout)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10, dropout=0.0):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], num_classes=num_classes, dropout=dropout)\n",
    "\n",
    "\n",
    "print(\"Model define\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda39cf-3f9b-4e59-8a5b-798ca6609519",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b143e4d2-bc89-4c68-8670-2127f447fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"%s\" % i for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a6c4ba-1fc8-452a-8c09-3c22c62374fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, classes, test_losses, test_accs,\n",
    "         misclassified_imgs, correct_imgs, is_last_epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss +=criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            is_correct = pred.eq(target.view_as(pred))\n",
    "            if is_last_epoch:\n",
    "              misclassified_inds = (is_correct==0).nonzero()[:,0]\n",
    "              for mis_ind in misclassified_inds:\n",
    "                if len(misclassified_imgs) == 25:\n",
    "                  break\n",
    "                misclassified_imgs.append({\n",
    "                    \"target\": target[mis_ind].cpu().numpy(),\n",
    "                    \"pred\": pred[mis_ind][0].cpu().numpy(),\n",
    "                    \"img\": data[mis_ind]\n",
    "                })\n",
    "              \n",
    "              correct_inds = (is_correct==1).nonzero()[:,0]\n",
    "              for ind in correct_inds:\n",
    "                if len(correct_imgs) == 25:\n",
    "                  break\n",
    "                correct_imgs.append({\n",
    "                    \"target\": target[ind].cpu().numpy(),\n",
    "                    \"pred\": pred[ind][0].cpu().numpy(),\n",
    "                    \"img\": data[ind]\n",
    "                })\n",
    "            correct += is_correct.sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    if test_acc >= 90.0:\n",
    "        classwise_acc(model, device, test_loader, classes)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset), test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d706096-f07c-42dd-b55c-7de9978a7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classwise_acc(model, device, test_loader, classes):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    # print class-wise test accuracies\n",
    "    print()\n",
    "    for i in range(10):\n",
    "      print('Accuracy of %5s : %2d %%' % (\n",
    "          classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "494aae07-229b-44be-ac04-6b59bf0a1f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from finn.util.visualization import showSrc, showInNetron\n",
    "from finn.util.basic import make_build_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a2c7448-fae7-4de9-9849-f2f48ae9693a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving './models/quentresnet18_weight2.pth' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7470e3ff7c70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"./models/quentresnet18_weight2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9275b452-dbe5-49ef-a158-82d16d2fe1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function ---\n",
    "def remove_export_handlers(model):\n",
    "    count = 0\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, \"export_handler\"):\n",
    "            module.export_handler = None\n",
    "            count += 1\n",
    "    print(f\"âœ… Removed export_handler from {count} Quant layers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54faa3a9-9e79-4fbc-9484-3678680eaf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model to cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Main export pipeline ---\n",
    "# Step 1: Construct model\n",
    "model = ResNet18(num_classes=10)\n",
    "\n",
    "# Step 2: Load weights\n",
    "trained_state_dict = torch.load(\"./models/quentresnet18_weight2.pth\", map_location='cpu')\n",
    "model.load_state_dict(trained_state_dict, strict=False)\n",
    "\n",
    "# Step 3: Remove export_handler from all quant layers\n",
    "#remove_export_handlers(model)\n",
    "\n",
    "# Step 4: Prepare for export\n",
    "model.eval()\n",
    "model.cpu()\n",
    "\n",
    "print(\"model to cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb241e4c-cbdd-4316-8b13-c5790966a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum= 0.9)\n",
    "test_losses, train_losses, test_accs, train_accs = [], [], [], []\n",
    "misclassified_imgs, correct_imgs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37196446-faae-4849-b5e8-28ff68888168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1255: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1758.)\n",
      "  return super(Tensor, self).rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 2.6973, Accuracy: 1070/10000 (10.70%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test for accuracy\n",
    "exact_acc = test(model, device, testloader, criterion, classes, test_losses,\n",
    "                 test_accs, misclassified_imgs, correct_imgs,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f2f89-cc4d-4a21-a05f-6108dbbbe560",
   "metadata": {},
   "source": [
    "# Export to QONNX and Conversion to FINN-ONNX\n",
    "\n",
    "ONNX is an open format built to represent machine learning models, and the FINN compiler expects an ONNX model as input. We'll now export our network into ONNX to be imported and used in FINN for the next notebooks. Note that the particular ONNX representation used for FINN differs from standard ONNX, you can read more about this here.\n",
    "\n",
    "You can see below how we export a trained network in Brevitas into a FINN-compatible ONNX representation (QONNX). QONNX is the format we can export from Brevitas, to feed it into the FINN compiler, we will need to make a conversion to the FINN-ONNX format which is the intermediate representation the compiler works on. The conversion of the FINN-ONNX format is a FINN compiler transformation and to be able to apply it to our model, we will need to wrap it into ModelWrapper. This is a wrapper around the ONNX model which provides several helper functions to make it easier to work with the model. Then we can call the conversion function to obtain the model in FINN-ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47cb0896-ba1f-4d44-8e86-1f63993abac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/home/pamela/finn/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to hardware/quantresnet18_weight2_files/quantresnet18_weight2.onnx\n"
     ]
    }
   ],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "ready_model_filename = \"hardware/quantresnet18_weight2_files/quantresnet18_weight2.onnx\"\n",
    "input_shape = (1, 3, 32, 32)\n",
    "\n",
    "# create a QuantTensor instance to mark input as bipolar during export\n",
    "input_a = np.random.randint(0, 1, size=input_shape).astype(np.float32)\n",
    "input_a = 2 * input_a - 1\n",
    "scale = 1.0\n",
    "input_t = torch.from_numpy(input_a * scale)\n",
    "\n",
    "#Move to CPU before export\n",
    "model.cpu()\n",
    "\n",
    "# Export to ONNX\n",
    "export_qonnx(\n",
    "    model, export_path=ready_model_filename, input_t=input_t\n",
    ")\n",
    "\n",
    "# clean-up\n",
    "qonnx_cleanup(ready_model_filename, out_file=ready_model_filename)\n",
    "\n",
    "# ModelWrapper\n",
    "model = ModelWrapper(ready_model_filename)\n",
    "# Setting the input datatype explicitly because it doesn't get derived from the export function\n",
    "model.set_tensor_datatype(model.graph.input[0].name, DataType[\"BIPOLAR\"])\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model.save(ready_model_filename)\n",
    "\n",
    "print(\"Model saved to %s\" % ready_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ff45699-e55e-42cf-881e-d277c9eaa2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving 'hardware/quantresnet18_weight2_files/quantresnet18_weight2.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7470c37236d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00145436-6739-4eb7-9848-6b17b12dbd6c",
   "metadata": {},
   "source": [
    "# Tidy-up transformations <a id='basic_trafo'></a>\n",
    "This section deals with some basic transformations, which are applied to the model like a kind of \"tidy-up\" to make it easier to be processed. They do not appear in the diagram above, but they are applied in many steps in the FINN flow to postprocess the model after a transformation and/or to prepare it for the next transformation.\n",
    "\n",
    "These transformations are:\n",
    "* GiveUniqueNodeNames\n",
    "* GiveReadableTensorNames\n",
    "* InferShapes\n",
    "* InferDataTypes\n",
    "* FoldConstants\n",
    "* RemoveStaticGraphInputs\n",
    "\n",
    "In the first two transformations (GiveUniqueNodeNames, GiveReadableTensorNames) the nodes in the graph are first given unique (by enumeration) names, then the tensors are given human-readable names (based on the node names). The following two transformations (InferShapes, InferDataTypes) derive the shapes and data types of the tensors from the model properties and set them in the ValueInfo of the model. These transformations can almost always be applied without negative effects and do not affect the structure of the graph, ensuring that all the information needed is available.\n",
    "\n",
    "The next listed transformation is FoldConstants, which performs constant folding. It identifies a node with constant inputs and determines its output. The result is then set as constant-only inputs for the following node and the old node is removed. Although this transformation changes the structure of the model, it is a transformation that is usually always desired and can be applied to any model. And finally, we have RemoveStaticGraphInputs to remove any top-level graph inputs that already have ONNX initializers associated with them.\n",
    "\n",
    "These transformations can be imported and applied as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5c5035a-24ce-4cd7-9a4d-0874a645c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "model.save(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "466889b7-d2b7-4a22-9648-81f093780705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving 'hardware/quantresnet18_weight2_files/quantresnet18_weight2_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7470e3ff7af0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e2fc1-a593-4c3a-a620-438aaab33ba6",
   "metadata": {},
   "source": [
    "# Adding Pre- and Postprocessing\n",
    "\n",
    "In many cases, it's common to apply some preprocessing to the raw data in a machine learning framework prior to training. For image classification networks, this may include conversion of raw 8-bit RGB values into floating point values between 0 and 1. Similarly, at the output of the network some postprocessing may be performed during deployment, such as extracting the indices of the classifications with the largest value (top-K indices).\n",
    "\n",
    "In FINN, we can bake some of these pre/postprocessing operatings into the graph, and in some cases these can be highly beneficial for performance by allowing our accelerator to directly consume raw data instead of going through CPU preprocessing.\n",
    "\n",
    "We'll demonstrate this for our small image classification network as follows. Brevitas preprocesses BNN-PYNQ network inputs with torchvision.transforms.ToTensor() prior to training, which converts 8-bit RGB values into floats between 0 and 1 by dividing the input by 255. We can achieve the same effect in FINN by exporting a single-node ONNX graph for division by 255 (which already exists as finn.util.pytorch.ToTensor and merging this with our original model. Finally, we're going to mark our input tensor as 8-bit to let FINN know which level of precision to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a9a3af5-ca80-4ffb-807f-74bb8447827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pamela/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "model = ModelWrapper(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_tidy.onnx\")\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = \"hardware/quantresnet18_weight2_files/quantresnet18_weight2_preproc.onnx\"\n",
    "export_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name)\n",
    "qonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "pre_model = pre_model.transform(ConvertQONNXtoFINN())\n",
    "\n",
    "# join preprocessing and core model\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\n",
    "model.save(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_preproc.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33c6749b-4bef-4e91-9748-c2fa258254c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving 'hardware/quantresnet18_weight2_files/quantresnet18_weight2_preproc.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7470c3723b80>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_preproc.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea50073-a040-4f71-99b9-32c1c5a4f1f6",
   "metadata": {},
   "source": [
    "You can observe two changes in the graph above: a Div node has appeared in the beginning to perform the input preprocessing, and the global_in tensor now has a quantization annotation to mark it as an unsigned 8-bit value.\n",
    "\n",
    "For the postprocessing we'll insert a TopK node for k=1 at the end of our graph. This will extract the index (class number) for the largest-valued output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2bdcb92-50bb-4834-9488-90f118a510ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "chkpt_name = \"hardware/quantresnet18_weight2_files/quantresnet18_weight2_pre_post.onnx\"\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(chkpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a41ebbb-29dd-4793-942b-a47fc68fc17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving 'hardware/quantresnet18_weight2_files/quantresnet18_weight2_pre_post.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7470c37244f0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_pre_post.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b28300-7a15-491b-9222-3a1a91ede4dd",
   "metadata": {},
   "source": [
    "# Streamlining\n",
    "\n",
    "Streamlining is a transformation containing several sub-transformations. The goal of streamlining is to eliminate floating point operations by moving them around, then collapsing them into one operation and in the last step transform them into multi-thresholding nodes. For more information on the theoretical background of this, see this paper.\n",
    "\n",
    "Let's have a look at which sub-transformations Streamline consists of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f56b857-ae99-423c-a221-162c8a305e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Streamline(Transformation):\n",
      "    \"\"\"Apply the streamlining transform, see arXiv:1709.04060.\"\"\"\n",
      "\n",
      "    def apply(self, model):\n",
      "        streamline_transformations = [\n",
      "            ConvertSubToAdd(),\n",
      "            ConvertDivToMul(),\n",
      "            BatchNormToAffine(),\n",
      "            ConvertSignToThres(),\n",
      "            MoveMulPastMaxPool(),\n",
      "            MoveScalarLinearPastInvariants(),\n",
      "            AbsorbSignBiasIntoMultiThreshold(),\n",
      "            MoveAddPastMul(),\n",
      "            MoveScalarAddPastMatMul(),\n",
      "            MoveAddPastConv(),\n",
      "            MoveScalarMulPastMatMul(),\n",
      "            MoveScalarMulPastConv(),\n",
      "            MoveAddPastMul(),\n",
      "            CollapseRepeatedAdd(),\n",
      "            CollapseRepeatedMul(),\n",
      "            MoveMulPastMaxPool(),\n",
      "            AbsorbAddIntoMultiThreshold(),\n",
      "            FactorOutMulSignMagnitude(),\n",
      "            AbsorbMulIntoMultiThreshold(),\n",
      "            Absorb1BitMulIntoMatMul(),\n",
      "            Absorb1BitMulIntoConv(),\n",
      "            RoundAndClipThresholds(),\n",
      "        ]\n",
      "        for trn in streamline_transformations:\n",
      "            model = model.transform(trn)\n",
      "            model = model.transform(RemoveIdentityOps())\n",
      "            model = model.transform(GiveUniqueNodeNames())\n",
      "            model = model.transform(GiveReadableTensorNames())\n",
      "            model = model.transform(InferDataTypes())\n",
      "        return (model, False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "showSrc(Streamline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebbedbb6-fa67-4624-89f2-c94dfc438074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.streamline.reorder import MoveScalarLinearPastInvariants\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "\n",
    "model = ModelWrapper(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_pre_post.onnx\")\n",
    "# move initial Mul (from preproc) past the Reshape\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "# streamline\n",
    "model = model.transform(Streamline())\n",
    "model.save(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b19433e-b68f-4472-8a66-3a34622b8742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving 'hardware/quantresnet18_weight2_files/quantresnet18_weight2_streamlined.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7470c3a641f0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5018bd5-d37b-4b27-8a0e-813c8c1fdae3",
   "metadata": {},
   "source": [
    "You can see that the network has become simplified considerably compared to the previous step -- a lot of nodes have disappeared between the `MatMul` layers. \n",
    "\n",
    "**The current implementation of streamlining is highly network-specific and may not work for your network if its topology is very different than the example network here. We hope to rectify this in future releases.**\n",
    "\n",
    "Our example network is a quantized network with 1-bit bipolar (-1, +1 values) precision, and we want FINN to implement them as XNOR-popcount operations [as described in the original FINN paper](https://arxiv.org/pdf/1612.07119). For this reason, after streamlining, the resulting bipolar matrix multiplications are converted into xnorpopcount operations. This transformation produces operations that are again collapsed and converted into thresholds. This procedure is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a0ae595-428f-4722-af15-7fcf92a3352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "from finn.transformation.streamline.round_thresholds import RoundAndClipThresholds\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(absorb.AbsorbAddIntoMultiThreshold())\n",
    "model = model.transform(absorb.AbsorbMulIntoMultiThreshold())\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(RoundAndClipThresholds())\n",
    "\n",
    "# bit of tidy-up\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "model.save(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_for_hw_conversion.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d7b7494-6a5d-41cd-8c3b-b3d2b390e722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving 'hardware/quantresnet18_weight2_files/quantresnet18_weight2_for_hw_conversion.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7470cc15ca60>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_for_hw_conversion.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f509c-537e-4be6-b414-1612bddd8387",
   "metadata": {},
   "source": [
    "Observe the pairs of `XnorPopcountmatMul` and `MultiThreshold` layers following each other -- this is the particular pattern that the next step will be looking for in order to convert them to hardware (HW) layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca645d-21bd-4efa-b6c7-ad1676828ea3",
   "metadata": {},
   "source": [
    "# Conversion to HW layers\n",
    "\n",
    "Converts the nodes to HW layers, these layers are abstraction layers that do not directly correspond to an HLS or Verilog implementation but they will be converted in either one later in the flow. In our case this transformation converts pairs of binary XnorPopcountMatMul layers to MVAU layers (matrix vector activation unit). Any immediately following MultiThreshold layers will also be absorbed into the MVAU.\n",
    "\n",
    "Below is the code for the transformation and the network is visualized using netron to create the new structure with MVAU nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3496a43b-110e-4e68-88cd-595ac5788625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "model = ModelWrapper(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_for_hw_conversion.onnx\")\n",
    "model = model.transform(to_hw.InferBinaryMatrixVectorActivation())\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hw.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hw.InferThresholdingLayer())\n",
    "model.save(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_for_layers.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff2c71f4-f74d-42a1-a47d-34c0191e6959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving 'hardware/quantresnet18_weight2_files/quantresnet18_weight2_for_layers.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7470cc102fb0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_for_layers.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503baa5-d194-4d64-8f92-6bc44e411936",
   "metadata": {},
   "source": [
    "# Creating a Dataflow Partition\n",
    "\n",
    "In the graph above, you can see that there is a mixture of FINN HW layers (MVAU and Thresholding) with one regular ONNX layers (Reshape). To create a bitstream, FINN needs a model with only HW layers. In order to achieve this, we will use the CreateDataflowPartition transformation to create a \"dataflow partition\" in this graph, separating out the HLS layers into another model, and replacing them with a placeholder layer called StreamingDataflowPartition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd5091fb-f344-49f7-b23d-62abe9f38eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.create_dataflow_partition import CreateDataflowPartition\n",
    "\n",
    "model = ModelWrapper(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_for_layers.onnx\")\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26ec9750-d39d-4a2b-8379-2cd2f71c6ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving 'hardware/quantresnet18_weight2_files/quantresnet18_weight2_dataflow_parent.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7470d15a8850>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cbd807-853a-40f1-93f2-abe7d0e0b6dc",
   "metadata": {},
   "source": [
    "We can see that the MVAU instances and the Thresholding in the beginning have all been replaced with a single StreamingDataflowPartition, which has an attribute model that points to the extracted, HW dataflow-only graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecda7f08-303f-48ed-ae54-79bc06305812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.custom_op.registry import getCustomOp\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e82bbaa4-0c78-47aa-8633-568f0346c86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving '/tmp/finn_dev_pamela/dataflow_partition_27flq59v/partition_0.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7470cc1014b0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(dataflow_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91addbe-38af-49d8-8744-979f776c1214",
   "metadata": {},
   "source": [
    "We can see all the extracted MVAU instances and the Thresholding have been moved to the child (dataflow) model. We will load the child model with ModelWrapper and continue working on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bee206e5-7326-4ca5-82bd-833e962e7570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(dataflow_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb5b37-9be7-47e3-920b-ff332e5efb1d",
   "metadata": {},
   "source": [
    "# Specialize layers\n",
    "\n",
    "The network is converted to HW abstraction layers and we have excluded the non-HW layers to continue with the processing of the model. HW abstraction layers are abstract (placeholder) layers that can be either implemented in HLS or as an RTL module using FINN. In the next flow step, we convert each of these layers to either an HLS or RTL variant by calling the SpecializeLayers transformation. It is possible to let the FINN flow know a preference for the implementation style {\"hls\", \"rtl\"} and depending on the layer type this wish will be fulfilled or it will be set to a reasonable default. In the tfc example, we will set all layers to their HLS variants. To showcase how to set the preferred implementation, we will set the node attribute in the Thresholding layer to \"hls\", for the MVAUs and the LabelSelect we will leave this node attribute empty and in this case by default it will be set to HLS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a421497-1b93-4657-9794-bef7b52dac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_node = model.get_nodes_by_op_type(\"Thresholding\")[0]\n",
    "thresh_node_inst = getCustomOp(thresh_node)\n",
    "thresh_node_inst.set_nodeattr(\"preferred_impl_style\", \"hls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b3fbed-add1-48e6-adba-aba57b7f5f16",
   "metadata": {},
   "source": [
    "We'll define two helper variables that describe the Xilinx FPGA part name and the PYNQ board name that we are targeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e31507f-a258-48dc-96a8-e35fd1c2e401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Ultra96', 'Ultra96-V2', 'Pynq-Z1', 'Pynq-Z2', 'ZCU102', 'ZCU104', 'ZCU111', 'RFSoC2x2', 'RFSoC4x2', 'KV260_SOM'])\n"
     ]
    }
   ],
   "source": [
    "# print the names of the supported PYNQ boards\n",
    "from finn.util.basic import pynq_part_map\n",
    "print(pynq_part_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e495c19b-858d-469b-89d9-5f1daab4dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this if you have a different PYNQ board, see list above\n",
    "pynq_board = \"Pynq-Z1\"\n",
    "fpga_part = pynq_part_map[pynq_board]\n",
    "target_clk_ns = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b02a6c1-5f2c-4049-8a91-a35e07e6ae61",
   "metadata": {},
   "source": [
    "Then we will call SpecializeLayers to convert each HW abstraction layer to (in this case) an HLS variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81549908-ac80-4142-91ba-240ce4d515a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "model = model.transform(SpecializeLayers(fpga_part))\n",
    "\n",
    "model.save(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_specialize_layers.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19ef8a7d-4f95-4a97-bf5f-e395a046d87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving 'hardware/quantresnet18_weight2_files/quantresnet18_weight2_specialize_layers.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7470cebaf3d0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_specialize_layers.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90c77f-ac15-4119-9219-9c7d621c3fdc",
   "metadata": {},
   "source": [
    "Each node type has now a suffix (_hls) and the module ( finn.custom_op.fpgadataflow.hls also indicates that that the HLS variant of the layer is selected. We can now proceed by adjusting the parallelism of each node to customize the performance and resource usage.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2bbd4d-f255-4eac-bc88-45b5a72a0bd2",
   "metadata": {},
   "source": [
    "#  Folding: Adjusting the Parallelism\n",
    "\n",
    "Folding in FINN describes how much a layer is time-multiplexed in terms of execution resources. There are several folding factors for each layer, controlled by the PE (parallelization over outputs) and SIMD (parallelization over inputs) parameters as described by the original FINN paper. The higher the PE and SIMD values are set, the faster the generated accelerator will run, and the more FPGA resources it will consume.\n",
    "\n",
    "Each MVAU_hls node has two attributes that specify the degree of folding, PE and SIMD. In all nodes the values for these attributes are set as default to 1, which would correspond to a maximum folding (time multiplexing) and thus minimum performance.\n",
    "\n",
    "Since the folding parameters are node attributes, they can be easily accessed and changed using a helper function of the ModelWrapper. But first we take a closer look at one of the nodes that implement a Matrix-Vector-Activation operation. This is where the Netron visualization helps us, in the above diagram we can see that the model contains four MVAUs. So as an example we extract the second node of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191fdfc-0641-48c6-885e-2ed037873731",
   "metadata": {},
   "source": [
    "We can use the higher-level CustomOp wrappers for this node. These wrappers provide easy access to specific properties of these nodes, such as the folding factors (PE and SIMD). Above, we have already used this abstraction to set the node attribute of the Thresholding HW layer. Let's have a look at which node attributes are defined by the CustomOp wrapper, and adjust the SIMD and PE attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18fe2bce-2a72-4104-b772-3701201ab2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomOp wrapper is of class Thresholding_hls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mem_mode': ('s',\n",
       "  False,\n",
       "  'internal_decoupled',\n",
       "  {'internal_decoupled', 'internal_embedded'}),\n",
       " 'ram_style': ('s', False, 'distributed', {'block', 'distributed'}),\n",
       " 'runtime_writeable_weights': ('i', False, 0, {0, 1}),\n",
       " 'PE': ('i', True, 0),\n",
       " 'NumChannels': ('i', True, 0),\n",
       " 'numSteps': ('i', True, 1),\n",
       " 'inputDataType': ('s', True, ''),\n",
       " 'weightDataType': ('s', True, ''),\n",
       " 'outputDataType': ('s', True, ''),\n",
       " 'numInputVectors': ('ints', False, [1]),\n",
       " 'ActVal': ('i', False, 0),\n",
       " 'backend': ('s', True, 'fpgadataflow'),\n",
       " 'preferred_impl_style': ('s', False, '', {'', 'hls', 'rtl'}),\n",
       " 'code_gen_dir_ipgen': ('s', False, ''),\n",
       " 'ipgen_path': ('s', False, ''),\n",
       " 'ip_path': ('s', False, ''),\n",
       " 'ip_vlnv': ('s', False, ''),\n",
       " 'exec_mode': ('s', False, '', {'', 'cppsim', 'rtlsim'}),\n",
       " 'cycles_rtlsim': ('i', False, 0),\n",
       " 'cycles_estimate': ('i', False, 0),\n",
       " 'rtlsim_trace': ('s', False, ''),\n",
       " 'res_estimate': ('s', False, ''),\n",
       " 'res_synth': ('s', False, ''),\n",
       " 'rtlsim_so': ('s', False, ''),\n",
       " 'slr': ('i', False, -1),\n",
       " 'mem_port': ('s', False, ''),\n",
       " 'partition_id': ('i', False, 0),\n",
       " 'device_id': ('i', False, 0),\n",
       " 'inFIFODepths': ('ints', False, [2]),\n",
       " 'outFIFODepths': ('ints', False, [2]),\n",
       " 'output_hook': ('s', False, ''),\n",
       " 'io_chrc_in': ('t', False, array([], dtype=int32)),\n",
       " 'io_chrc_out': ('t', False, array([], dtype=int32)),\n",
       " 'io_chrc_period': ('i', False, 0),\n",
       " 'io_chrc_pads_in': ('ints', False, []),\n",
       " 'io_chrc_pads_out': ('ints', False, []),\n",
       " 'code_gen_dir_cppsim': ('s', False, ''),\n",
       " 'executable_path': ('s', False, ''),\n",
       " 'res_hls': ('s', False, '')}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc0 = model.graph.node[0]\n",
    "fc0w = getCustomOp(fc0)\n",
    "\n",
    "print(\"CustomOp wrapper is of class \" + fc0w.__class__.__name__)\n",
    "\n",
    "fc0w.get_nodeattr_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e964ad3f-a8b1-4453-9362-2cbdd0e2758c",
   "metadata": {},
   "source": [
    "We can see that the PE and SIMD are listed as node attributes, as well as the depths of the FIFOs that will be inserted between consecutive layers, and all can be adjusted using set_nodeattr subject to certain constraints. There are also a lot of additional attributes that can be set for this node type. In this notebook we are setting the folding factors and FIFO depths manually but it is possible to use FINN transformations for this (SetFolding and InsertAndSetFIFODepths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf8aaf3e-4ae2-4109-9e54-e7b250bdb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layers = model.get_nodes_by_op_type(\"MVAU_hls\")\n",
    "# (PE, SIMD, in_fifo_depth, out_fifo_depth, ramstyle) for each layer\n",
    "config = [\n",
    "    (16, 49, [16], [64], \"block\"),\n",
    "    (8, 8, [64], [64], \"auto\"),\n",
    "    (8, 8, [64], [64], \"auto\"),\n",
    "    (10, 8, [64], [10], \"distributed\"),\n",
    "]\n",
    "for fcl, (pe, simd, ififo, ofifo, ramstyle) in zip(fc_layers, config):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepths\", ififo)\n",
    "    fcl_inst.set_nodeattr(\"outFIFODepths\", ofifo)\n",
    "    fcl_inst.set_nodeattr(\"ram_style\", ramstyle)\n",
    "    \n",
    "# set parallelism for input quantizer to be same as first layer's SIMD\n",
    "inp_qnt_node = model.get_nodes_by_op_type(\"Thresholding_hls\")[0]\n",
    "inp_qnt = getCustomOp(inp_qnt_node)\n",
    "inp_qnt.set_nodeattr(\"PE\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a49c7d-ab58-4c39-befa-3af023c8872b",
   "metadata": {},
   "source": [
    "We are setting PE and SIMD so that each layer has a total folding of 16.\n",
    "\n",
    "Besides PE and SIMD three other node attributes are set. ram_style specifies how the weights are to be stored (BRAM, LUTRAM, and so on). It can be selected explicitly or with the option auto you can let Vivado decide. inFIFODepths and outFIFODepths specifies the FIFO depths that is needed by the node from the surrounding FIFOs. These attributes are used in the transformation 'InsertFIFO' to insert the appropriate FIFOs between the nodes, which will be automatically called as part of the hardware build process.\n",
    "\n",
    "In previous versions of FINN we had to call transformations to insert data width converters, FIFOs and TLastMarker manually at this step. This is no longer needed, as all this is taken care of by the ZynqBuild or VitisBuild transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c9c4d0d-f5ab-43fc-8773-4a14d9c0a5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumChannels: 64\n",
      "PE: 49\n"
     ]
    }
   ],
   "source": [
    "inp_qnt_node = model.get_nodes_by_op_type(\"Thresholding_hls\")[0]\n",
    "inp_qnt = getCustomOp(inp_qnt_node)\n",
    "print(\"NumChannels:\", inp_qnt.get_nodeattr(\"NumChannels\"))\n",
    "print(\"PE:\", inp_qnt.get_nodeattr(\"PE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4fdd9d61-1cad-4380-87db-cc8d7c55903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_qnt.set_nodeattr(\"PE\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1533e125-e5c0-4752-b62c-0b2caf4d7a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"PE:\", inp_qnt.get_nodeattr(\"PE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3bad1788-bc11-4590-9fdb-0c57229d71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_set_folding_factors.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d53de09-7297-4e4d-bd16-2e09061e26ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving 'hardware/quantresnet18_weight2_files/quantresnet18_weight2_set_folding_factors.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x747171a89e10>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_set_folding_factors.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6508af-c495-4327-a927-45d1f3eade04",
   "metadata": {},
   "source": [
    "# Hardware Build\n",
    "\n",
    "We're finally ready to start generating hardware from our network. Depending on whether you want to target a Zynq or Alveo platform, FINN offers two transformations to build the accelerator, integrate into an appropriate shell and build a bitfile. These are ZynqBuild and VitisBuild for Zynq and Alveo, respectively. In this notebook we'll demonstrate the ZynqBuild as these boards are more common and it's much faster to complete bitfile generation for the smaller FPGAs found on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0c35f-ea6d-447a-8db8-5e1e18b64f69",
   "metadata": {},
   "source": [
    "In previous versions of FINN, we had to manually go through several steps to generate HLS/RTL code, stitch IP, create a PYNQ project and run synthesis. All these steps are now performed by the `ZynqBuild` transform (or the `VitisBuild` transform for Alveo). **As this involves calling HLS synthesis and Vivado synthesis, this transformation will run for some time (up to half an hour depending on your PC).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7019dc1-3179-4c3a-97a5-7ce7aee689a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pamela/finn/src/finn/transformation/fpgadataflow/floorplan.py:107: UserWarning: 3 nodes have no entry in the provided floorplan, SLR was set to -1\n",
      "  warnings.warn(\n",
      "/home/pamela/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:234: UserWarning: Input FIFO for IODMA_hls_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/pamela/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:294: UserWarning: Output FIFO for Thresholding_hls_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/pamela/finn/src/finn/transformation/fpgadataflow/create_stitched_ip.py:290: UserWarning: First node is not StreamingFIFO or IODMA.\n",
      "                You may experience incorrect stitched-IP rtlsim or hardware\n",
      "                behavior. It is strongly recommended to insert FIFOs prior to\n",
      "                calling CreateStitchedIP.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "model = ModelWrapper(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_set_folding_factors.onnx\")\n",
    "model = model.transform(ZynqBuild(platform = pynq_board, period_ns = target_clk_ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4a209efb-c5f6-4246-9518-b37e99027951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "model = model.transform(MakePYNQDriver(\"zynq-iodma\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f4bc14e-edf8-44aa-93fb-112d44fd52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_post_synthesis.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a7e5846a-b868-4a74-9749-cf73ab74a2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving 'hardware/quantresnet18_weight2_files/quantresnet18_weight2_post_synthesis.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7470cc171270>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_post_synthesis.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a67fdff-a1c5-4400-8bff-fb9acc365333",
   "metadata": {},
   "source": [
    "# Examining the generated outputs\n",
    "\n",
    "Let's start by viewing the post-synthesis model in Netron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "125e1fb0-608e-4685-9b5a-9d2c6b92bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving '/tmp/finn_dev_pamela/dataflow_partition_e9yym5a5/partition_2.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x747171b1cb20>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_post_synthesis.onnx\")\n",
    "sdp_node_middle = getCustomOp(model.graph.node[1])\n",
    "postsynth_layers = sdp_node_middle.get_nodeattr(\"model\")\n",
    "\n",
    "showInNetron(postsynth_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c0fbf26e-1f18-4fd8-a621-bc45498aee6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[key: \"floorplan_json\"\n",
       "value: \"/tmp/finn_dev_pamela/vitis_floorplan_xhyg7if4/floorplan.json\"\n",
       ", key: \"vivado_stitch_proj\"\n",
       "value: \"/tmp/finn_dev_pamela/vivado_stitch_proj_5vub_qyk\"\n",
       ", key: \"clk_ns\"\n",
       "value: \"10\"\n",
       ", key: \"wrapper_filename\"\n",
       "value: \"/tmp/finn_dev_pamela/vivado_stitch_proj_5vub_qyk/finn_vivado_stitch_proj.gen/sources_1/bd/StreamingDataflowPartition_1/hdl/StreamingDataflowPartition_1_wrapper.v\"\n",
       ", key: \"vivado_stitch_vlnv\"\n",
       "value: \"xilinx_finn:finn:StreamingDataflowPartition_1:1.0\"\n",
       ", key: \"vivado_stitch_ifnames\"\n",
       "value: \"{\\\"clk\\\": [\\\"ap_clk\\\"], \\\"rst\\\": [\\\"ap_rst_n\\\"], \\\"s_axis\\\": [[\\\"s_axis_0\\\", 512]], \\\"m_axis\\\": [[\\\"m_axis_0\\\", 32]], \\\"aximm\\\": [], \\\"axilite\\\": []}\"\n",
       ", key: \"platform\"\n",
       "value: \"zynq-iodma\"\n",
       "]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper(postsynth_layers)\n",
    "model.model.metadata_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1790d6d4-1a00-44d8-8703-9a393d12d46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[key: \"floorplan_json\"\n",
       "value: \"/tmp/finn_dev_pamela/vitis_floorplan_xhyg7if4/floorplan.json\"\n",
       ", key: \"vivado_pynq_proj\"\n",
       "value: \"/tmp/finn_dev_pamela/vivado_zynq_proj_mkohv0_9\"\n",
       ", key: \"bitfile\"\n",
       "value: \"/tmp/finn_dev_pamela/vivado_zynq_proj_mkohv0_9/resizer.bit\"\n",
       ", key: \"hw_handoff\"\n",
       "value: \"/tmp/finn_dev_pamela/vivado_zynq_proj_mkohv0_9/resizer.hwh\"\n",
       ", key: \"vivado_synth_rpt\"\n",
       "value: \"/tmp/finn_dev_pamela/vivado_zynq_proj_mkohv0_9/synth_report.xml\"\n",
       ", key: \"platform\"\n",
       "value: \"zynq-iodma\"\n",
       ", key: \"pynq_driver_dir\"\n",
       "value: \"/tmp/finn_dev_pamela/pynq_driver_pftt734i\"\n",
       "]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper(\"hardware/quantresnet18_weight2_files/quantresnet18_weight2_post_synthesis.onnx\")\n",
    "model.model.metadata_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f304c37d-8997-4da9-b775-8c355cc922e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finn_zynq_link.cache\t      finn_zynq_link.srcs  resizer.hwh\n",
      "finn_zynq_link.gen\t      finn_zynq_link.xpr   synth_project.sh\n",
      "finn_zynq_link.hw\t      ip_config.tcl\t   synth_report.xml\n",
      "finn_zynq_link.ip_user_files  NA\t\t   vivado.jou\n",
      "finn_zynq_link.runs\t      resizer.bit\t   vivado.log\n"
     ]
    }
   ],
   "source": [
    "! ls {model.get_metadata_prop(\"vivado_pynq_proj\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30541083-be12-458f-be70-f3a216a24f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
